#!/usr/bin/env python3
"""
æµ‹è¯•æ¡ä»¶å›è°ƒä¼˜åŒ–çš„æ•ˆæœ
éªŒè¯ï¼š
1. ä¸éœ€è¦checkpointæ—¶ï¼Œä¸è§¦å‘å›è°ƒ
2. éœ€è¦checkpointæ—¶ï¼Œæ­£å¸¸è§¦å‘å›è°ƒ
3. æ€§èƒ½æå‡æ˜æ˜¾
"""

import torch
import torch.nn as nn
import time
import sys
import os

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pccheck.layerwise_checkpoint.complete_integration import LayerwiseCheckpointTrainer


class SimpleModel(nn.Module):
    """ç®€å•çš„æµ‹è¯•æ¨¡å‹"""
    def __init__(self, input_size=128, hidden_size=256, num_layers=5):
        super().__init__()
        layers = []
        layers.append(nn.Linear(input_size, hidden_size))
        layers.append(nn.ReLU())
        
        for _ in range(num_layers - 2):
            layers.append(nn.Linear(hidden_size, hidden_size))
            layers.append(nn.ReLU())
        
        layers.append(nn.Linear(hidden_size, 10))
        self.model = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.model(x)


def test_callback_behavior():
    """æµ‹è¯•1ï¼šéªŒè¯å›è°ƒè¡Œä¸º"""
    print("="*80)
    print("æµ‹è¯•1ï¼šéªŒè¯æ¡ä»¶å›è°ƒæœºåˆ¶")
    print("="*80)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = SimpleModel().to(device)
    
    # è®¡æ•°å™¨
    callback_count = [0]
    
    # åˆ›å»º trainer
    trainer = LayerwiseCheckpointTrainer(
        model=model,
        optimizer_class=torch.optim.Adam,
        optimizer_kwargs={'lr': 0.001},
        checkpoint_dir='/tmp/test_callback_opt',
        use_pccheck=False,  # ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼
        verbose=False
    )
    
    # é‡å†™å›è°ƒè®¡æ•°
    original_callback = trainer._optimizer_callback
    def counting_callback(*args, **kwargs):
        callback_count[0] += 1
        return original_callback(*args, **kwargs)
    trainer.optimizer.callback = counting_callback
    
    criterion = nn.CrossEntropyLoss()
    
    print("\næ­¥éª¤1: è¿è¡Œ10æ­¥è®­ç»ƒï¼Œæ¯5æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹")
    print("-"*80)
    
    for step in range(1, 11):
        # ç”Ÿæˆå‡æ•°æ®
        inputs = torch.randn(4, 128).to(device)
        labels = torch.randint(0, 10, (4,)).to(device)
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ£€æŸ¥ç‚¹
        need_checkpoint = (step % 5 == 0)
        
        # è®­ç»ƒ
        loss = trainer.train_step(
            inputs, labels, criterion,
            enable_checkpoint=need_checkpoint
        )
        
        print(f"  Step {step:2d}: enable_checkpoint={need_checkpoint}, "
              f"loss={loss:.4f}")
        
        if need_checkpoint:
            trainer.finalize_checkpoint()
    
    print(f"\nâœ“ æ€»å…±è§¦å‘å›è°ƒæ¬¡æ•°: {callback_count[0]}")
    
    # è·å–å±‚æ•°
    num_layers = len(trainer.update_order)
    print(f"âœ“ æ¨¡å‹æ€»å±‚æ•°: {num_layers}")
    
    expected_callbacks = 2 * num_layers  # Step 5 å’Œ Step 10
    print(f"âœ“ é¢„æœŸå›è°ƒæ¬¡æ•°: {expected_callbacks} (2æ¬¡æ£€æŸ¥ç‚¹ Ã— {num_layers}å±‚)")
    
    if callback_count[0] == expected_callbacks:
        print("\nâœ… æµ‹è¯•é€šè¿‡ï¼å›è°ƒåªåœ¨éœ€è¦æ—¶è§¦å‘ã€‚")
    else:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥ï¼é¢„æœŸ {expected_callbacks} æ¬¡ï¼Œå®é™… {callback_count[0]} æ¬¡")
    
    trainer.shutdown()
    
    return callback_count[0] == expected_callbacks


def test_performance_improvement():
    """æµ‹è¯•2ï¼šå¯¹æ¯”æ€§èƒ½æå‡"""
    print("\n" + "="*80)
    print("æµ‹è¯•2ï¼šæ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("="*80)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # æµ‹è¯•é…ç½®
    num_steps = 100
    checkpoint_freq = 10  # æ¯10æ­¥ä¿å­˜ä¸€æ¬¡
    
    print(f"\né…ç½®: {num_steps}æ­¥è®­ç»ƒ, æ¯{checkpoint_freq}æ­¥ä¿å­˜æ£€æŸ¥ç‚¹")
    print("-"*80)
    
    # ========== æµ‹è¯•1ï¼šä¼˜åŒ–åçš„ç‰ˆæœ¬ï¼ˆæ¡ä»¶å›è°ƒï¼‰ ==========
    print("\n[1] ä¼˜åŒ–åç‰ˆæœ¬ (æ¡ä»¶å›è°ƒ)...")
    
    model1 = SimpleModel().to(device)
    trainer1 = LayerwiseCheckpointTrainer(
        model=model1,
        optimizer_class=torch.optim.Adam,
        optimizer_kwargs={'lr': 0.001},
        checkpoint_dir='/tmp/test_callback_opt1',
        use_pccheck=False,
        verbose=False
    )
    
    criterion = nn.CrossEntropyLoss()
    
    start_time = time.time()
    for step in range(1, num_steps + 1):
        inputs = torch.randn(4, 128).to(device)
        labels = torch.randint(0, 10, (4,)).to(device)
        
        need_checkpoint = (step % checkpoint_freq == 0)
        loss = trainer1.train_step(
            inputs, labels, criterion,
            enable_checkpoint=need_checkpoint
        )
        
        if need_checkpoint:
            trainer1.finalize_checkpoint()
    
    optimized_time = time.time() - start_time
    trainer1.shutdown()
    
    print(f"  å®Œæˆæ—¶é—´: {optimized_time:.3f}ç§’")
    
    # ========== æµ‹è¯•2ï¼šæœªä¼˜åŒ–ç‰ˆæœ¬ï¼ˆæ¯æ­¥éƒ½å›è°ƒï¼‰ ==========
    print("\n[2] æœªä¼˜åŒ–ç‰ˆæœ¬ (æ¯æ­¥éƒ½å›è°ƒ)...")
    
    model2 = SimpleModel().to(device)
    trainer2 = LayerwiseCheckpointTrainer(
        model=model2,
        optimizer_class=torch.optim.Adam,
        optimizer_kwargs={'lr': 0.001},
        checkpoint_dir='/tmp/test_callback_opt2',
        use_pccheck=False,
        verbose=False
    )
    
    # ğŸ”¥ å¼ºåˆ¶ä½¿ç”¨ auto æ¨¡å¼ï¼ˆæ¯æ­¥éƒ½å›è°ƒï¼‰
    trainer2.optimizer.set_checkpoint_mode('auto')
    
    start_time = time.time()
    for step in range(1, num_steps + 1):
        inputs = torch.randn(4, 128).to(device)
        labels = torch.randint(0, 10, (4,)).to(device)
        
        # enable_checkpoint å‚æ•°åœ¨ auto æ¨¡å¼ä¸‹è¢«å¿½ç•¥
        loss = trainer2.train_step(
            inputs, labels, criterion,
            enable_checkpoint=False  # å³ä½¿è®¾ä¸ºFalseï¼Œautoæ¨¡å¼ä»ä¼šè§¦å‘
        )
        
        if step % checkpoint_freq == 0:
            trainer2.finalize_checkpoint()
    
    unoptimized_time = time.time() - start_time
    trainer2.shutdown()
    
    print(f"  å®Œæˆæ—¶é—´: {unoptimized_time:.3f}ç§’")
    
    # ========== ç»“æœå¯¹æ¯” ==========
    print("\n" + "="*80)
    print("æ€§èƒ½å¯¹æ¯”ç»“æœ")
    print("="*80)
    print(f"ä¼˜åŒ–åç‰ˆæœ¬:   {optimized_time:.3f}ç§’")
    print(f"æœªä¼˜åŒ–ç‰ˆæœ¬:   {unoptimized_time:.3f}ç§’")
    print(f"åŠ é€Ÿæ¯”:       {unoptimized_time/optimized_time:.2f}x")
    print(f"æ—¶é—´èŠ‚çœ:     {(unoptimized_time - optimized_time):.3f}ç§’ "
          f"({(1 - optimized_time/unoptimized_time)*100:.1f}%)")
    
    if optimized_time < unoptimized_time:
        improvement = (unoptimized_time - optimized_time) / unoptimized_time * 100
        print(f"\nâœ… ä¼˜åŒ–æˆåŠŸï¼æ€§èƒ½æå‡ {improvement:.1f}%")
        return True
    else:
        print(f"\nâš ï¸  æœªè§æ˜æ˜¾æ€§èƒ½æå‡")
        return False


def test_checkpoint_correctness():
    """æµ‹è¯•3ï¼šéªŒè¯æ£€æŸ¥ç‚¹æ­£ç¡®æ€§"""
    print("\n" + "="*80)
    print("æµ‹è¯•3ï¼šéªŒè¯æ£€æŸ¥ç‚¹ä¿å­˜æ­£ç¡®æ€§")
    print("="*80)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = SimpleModel().to(device)
    
    trainer = LayerwiseCheckpointTrainer(
        model=model,
        optimizer_class=torch.optim.Adam,
        optimizer_kwargs={'lr': 0.001},
        checkpoint_dir='/tmp/test_callback_correctness',
        use_pccheck=False,
        verbose=False
    )
    
    criterion = nn.CrossEntropyLoss()
    
    print("\nè¿è¡Œ5æ­¥è®­ç»ƒï¼Œæ¯2æ­¥ä¿å­˜æ£€æŸ¥ç‚¹...")
    
    for step in range(1, 6):
        inputs = torch.randn(4, 128).to(device)
        labels = torch.randint(0, 10, (4,)).to(device)
        
        need_checkpoint = (step % 2 == 0)
        loss = trainer.train_step(
            inputs, labels, criterion,
            enable_checkpoint=need_checkpoint
        )
        
        if need_checkpoint:
            trainer.finalize_checkpoint()
            print(f"  Step {step}: ä¿å­˜æ£€æŸ¥ç‚¹")
    
    # æ£€æŸ¥å…ƒæ•°æ®
    saved_checkpoints = list(trainer.metadata_manager.checkpoints.keys())
    print(f"\nä¿å­˜çš„æ£€æŸ¥ç‚¹: {saved_checkpoints}")
    
    expected_checkpoints = ['step_2', 'step_4']
    if saved_checkpoints == expected_checkpoints:
        print("âœ… æ£€æŸ¥ç‚¹ä¿å­˜æ­£ç¡®ï¼")
        result = True
    else:
        print(f"âŒ æ£€æŸ¥ç‚¹ä¸æ­£ç¡®ï¼é¢„æœŸ {expected_checkpoints}")
        result = False
    
    trainer.shutdown()
    return result


if __name__ == '__main__':
    print("\n" + "="*80)
    print("æ¡ä»¶å›è°ƒä¼˜åŒ–æµ‹è¯•")
    print("="*80)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test1_pass = test_callback_behavior()
    test2_pass = test_performance_improvement()
    test3_pass = test_checkpoint_correctness()
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("æµ‹è¯•æ€»ç»“")
    print("="*80)
    print(f"æµ‹è¯•1 (å›è°ƒè¡Œä¸º):     {'âœ… é€šè¿‡' if test1_pass else 'âŒ å¤±è´¥'}")
    print(f"æµ‹è¯•2 (æ€§èƒ½æå‡):     {'âœ… é€šè¿‡' if test2_pass else 'âš ï¸  æœªè§æå‡'}")
    print(f"æµ‹è¯•3 (æ£€æŸ¥ç‚¹æ­£ç¡®æ€§): {'âœ… é€šè¿‡' if test3_pass else 'âŒ å¤±è´¥'}")
    
    if test1_pass and test3_pass:
        print("\nğŸ‰ ä¼˜åŒ–å®ç°æˆåŠŸï¼")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦æ£€æŸ¥")
