#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PCCheck æ”¹è¿›æ•ˆæœå¯¹æ¯”å®éªŒ
Benchmark Comparison: Traditional vs Layerwise Checkpoint

å¯¹æ¯”ä¸‰ç§æ£€æŸ¥ç‚¹æ–¹æ³•ï¼š
1. ä¼ ç»Ÿ PyTorch æ£€æŸ¥ç‚¹ (torch.save)
2. åŸå§‹ PCCheck
3. æ”¹è¿›çš„åˆ†å±‚ PCCheck (Layerwise)

æµ‹é‡æŒ‡æ ‡ï¼š
- æ£€æŸ¥ç‚¹ä¿å­˜æ—¶é—´
- è®­ç»ƒååé‡ (samples/sec)
- å†…å­˜å³°å€¼
- æ€»è®­ç»ƒæ—¶é—´
- I/O å¼€é”€å æ¯”
"""

import os
import sys
import time
import json
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from typing import Dict, List, Tuple
import psutil
try:
    import GPUtil
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False
    print("[Warning] GPUtil not available, GPU memory monitoring disabled")
from datetime import datetime

# æ·»åŠ è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'pccheck'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'pccheck', 'layerwise_checkpoint'))

from checkpoint_eval.pccheck.layerwise_checkpoint.complete_integration import LayerwiseCheckpointTrainer
from checkpoint_eval.pccheck.chk_monitor import Chk_monitor
from checkpoint_eval.pccheck_utils import initialize, get_total_size, set_storage


class BenchmarkMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self, name: str):
        self.name = name
        self.checkpoint_times = []
        self.training_step_times = []
        self.memory_usage = []
        self.gpu_memory_usage = []
        self.total_time = 0.0
        self.throughput = 0.0
        self.num_samples = 0
        
    def add_checkpoint_time(self, time_ms: float):
        self.checkpoint_times.append(time_ms)
    
    def add_step_time(self, time_ms: float):
        self.training_step_times.append(time_ms)
    
    def record_memory(self):
        """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        process = psutil.Process()
        self.memory_usage.append(process.memory_info().rss / 1024**3)  # GB
        
        # GPU å†…å­˜
        if GPU_AVAILABLE:
            try:
                gpus = GPUtil.getGPUs()
                if gpus:
                    self.gpu_memory_usage.append(gpus[0].memoryUsed / 1024)  # GB
            except:
                pass
    
    def compute_statistics(self):
        """è®¡ç®—ç»Ÿè®¡æ•°æ®"""
        return {
            'name': self.name,
            'checkpoint': {
                'mean_ms': np.mean(self.checkpoint_times) if self.checkpoint_times else 0,
                'std_ms': np.std(self.checkpoint_times) if self.checkpoint_times else 0,
                'min_ms': np.min(self.checkpoint_times) if self.checkpoint_times else 0,
                'max_ms': np.max(self.checkpoint_times) if self.checkpoint_times else 0,
                'total_ms': np.sum(self.checkpoint_times) if self.checkpoint_times else 0,
                'count': len(self.checkpoint_times),
            },
            'training_step': {
                'mean_ms': np.mean(self.training_step_times) if self.training_step_times else 0,
                'std_ms': np.std(self.training_step_times) if self.training_step_times else 0,
            },
            'memory': {
                'peak_cpu_gb': max(self.memory_usage) if self.memory_usage else 0,
                'peak_gpu_gb': max(self.gpu_memory_usage) if self.gpu_memory_usage else 0,
                'mean_cpu_gb': np.mean(self.memory_usage) if self.memory_usage else 0,
            },
            'throughput': {
                'samples_per_sec': self.throughput,
                'total_samples': self.num_samples,
            },
            'total_time_sec': self.total_time,
            'checkpoint_overhead_percent': (np.sum(self.checkpoint_times) / 1000 / self.total_time * 100) if self.total_time > 0 else 0,
        }
    
    def print_summary(self):
        """æ‰“å°æ‘˜è¦"""
        stats = self.compute_statistics()
        print(f"\n{'='*80}")
        print(f"ğŸ“Š {stats['name']} - æ€§èƒ½æ‘˜è¦")
        print(f"{'='*80}")
        print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {stats['total_time_sec']:.2f} ç§’")
        print(f"ğŸš€ ååé‡: {stats['throughput']['samples_per_sec']:.2f} samples/sec")
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹ä¿å­˜:")
        print(f"   - å¹³å‡æ—¶é—´: {stats['checkpoint']['mean_ms']:.2f} ms")
        print(f"   - æ€»æ—¶é—´: {stats['checkpoint']['total_ms']/1000:.2f} ç§’")
        print(f"   - å¼€é”€å æ¯”: {stats['checkpoint_overhead_percent']:.2f}%")
        print(f"   - ä¿å­˜æ¬¡æ•°: {stats['checkpoint']['count']}")
        print(f"ğŸ“ˆ è®­ç»ƒæ­¥:")
        print(f"   - å¹³å‡æ—¶é—´: {stats['training_step']['mean_ms']:.2f} ms")
        print(f"ğŸ’» å†…å­˜:")
        print(f"   - CPU å³°å€¼: {stats['memory']['peak_cpu_gb']:.2f} GB")
        if stats['memory']['peak_gpu_gb'] > 0:
            print(f"   - GPU å³°å€¼: {stats['memory']['peak_gpu_gb']:.2f} GB")
        print(f"{'='*80}\n")


class TestModel(nn.Module):
    """æµ‹è¯•æ¨¡å‹ - å¯é…ç½®å¤§å°çš„ Transformer-like æ¨¡å‹"""
    
    def __init__(self, vocab_size=10000, d_model=512, nhead=8, num_layers=6, dim_feedforward=2048):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = nn.Embedding(512, d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc = nn.Linear(d_model, vocab_size)
        
    def forward(self, x):
        # x: (batch, seq_len)
        positions = torch.arange(x.size(1), device=x.device).unsqueeze(0)
        x = self.embedding(x) + self.pos_encoder(positions)
        x = self.transformer(x)
        x = self.fc(x)
        return x


def create_synthetic_dataset(num_samples=1000, seq_len=128, vocab_size=10000):
    """åˆ›å»ºåˆæˆæ•°æ®é›†"""
    print(f"ğŸ“¦ åˆ›å»ºåˆæˆæ•°æ®é›†: {num_samples} samples, seq_len={seq_len}")
    X = torch.randint(0, vocab_size, (num_samples, seq_len))
    y = torch.randint(0, vocab_size, (num_samples, seq_len))
    return TensorDataset(X, y)


def benchmark_traditional_checkpoint(
    model: nn.Module,
    train_loader: DataLoader,
    criterion,
    optimizer,
    device: str,
    checkpoint_freq: int,
    checkpoint_dir: str,
    num_steps: int = 100
) -> BenchmarkMetrics:
    """æµ‹è¯•ä¼ ç»Ÿ PyTorch æ£€æŸ¥ç‚¹"""
    
    print(f"\n{'='*80}")
    print(f"ğŸ”µ å¼€å§‹æµ‹è¯•: ä¼ ç»Ÿ PyTorch æ£€æŸ¥ç‚¹")
    print(f"{'='*80}")
    
    metrics = BenchmarkMetrics("Traditional PyTorch Checkpoint")
    model.train()
    
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    step = 0
    total_samples = 0
    start_time = time.time()
    
    data_iter = iter(train_loader)
    
    while step < num_steps:
        try:
            data, target = next(data_iter)
        except StopIteration:
            data_iter = iter(train_loader)
            data, target = next(data_iter)
        
        data, target = data.to(device), target.to(device)
        batch_size = data.size(0)
        
        # è®­ç»ƒæ­¥
        step_start = time.time()
        
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output.view(-1, output.size(-1)), target.view(-1))
        loss.backward()
        optimizer.step()
        
        step_time = (time.time() - step_start) * 1000
        metrics.add_step_time(step_time)
        
        total_samples += batch_size
        step += 1
        
        # æ£€æŸ¥ç‚¹ä¿å­˜
        if step % checkpoint_freq == 0:
            chk_start = time.time()
            
            checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_step_{step}.pth")
            torch.save({
                'step': step,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': loss.item(),
            }, checkpoint_path)
            
            chk_time = (time.time() - chk_start) * 1000
            metrics.add_checkpoint_time(chk_time)
            # å‡å°‘è¾“å‡ºé¢‘ç‡
            if step % (checkpoint_freq * 5) == 0:  # æ¯5æ¬¡æ£€æŸ¥ç‚¹è¾“å‡ºä¸€æ¬¡
                print(f"  âœ“ Step {step}: ä¿å­˜æ£€æŸ¥ç‚¹ ({chk_time:.2f} ms)")
        
        # è®°å½•å†…å­˜
        if step % 10 == 0:
            metrics.record_memory()
    
    total_time = time.time() - start_time
    metrics.total_time = total_time
    metrics.num_samples = total_samples
    metrics.throughput = total_samples / total_time
    
    return metrics


def benchmark_original_pccheck(
    model: nn.Module,
    train_loader: DataLoader,
    criterion,
    optimizer,
    device: str,
    checkpoint_freq: int,
    checkpoint_file: str,
    num_threads: int,
    max_async: int,
    num_steps: int = 100
) -> BenchmarkMetrics:
    """æµ‹è¯•åŸå§‹ PCCheck"""
    
    print(f"\n{'='*80}")
    print(f"ğŸŸ¢ å¼€å§‹æµ‹è¯•: åŸå§‹ PCCheck")
    print(f"{'='*80}")
    
    metrics = BenchmarkMetrics("Original PCCheck")
    model.train()
    
    # åˆå§‹åŒ– PCCheck Monitor
    print(f"ğŸ“ åˆå§‹åŒ– PCCheck Monitor:")
    
    # ä½¿ç”¨ initialize å‡½æ•°æ¥å‡†å¤‡ GPU æ•°ç»„
    gpu_ar, total_size = initialize(model, [optimizer], do_opt_step=False)
    
    print(f"   - æ¨¡å‹å¤§å°: {total_size/1e6:.2f}M å‚æ•°")
    print(f"   - Threads: {num_threads}, Max async: {max_async}")
    
    # è®¾ç½®å­˜å‚¨
    set_storage(model, [optimizer], gpu_ar)
    torch.cuda.empty_cache()
    
    # åˆ›å»º Chk_monitor
    c_lib_path = "/home/linzhicheng/code/pccheck/checkpoint_eval/pccheck/libtest_ssd.so"
    gpu_copy = True if device == 'cuda' else False
    
    monitor = Chk_monitor(
        c_lib_path,
        total_size,
        num_threads,
        max_async,
        gpu_copy,
        gpu_ar=gpu_ar,
        bsize=total_size // 4,
        model=model.state_dict(),
        optimizer=optimizer.state_dict(),
        memory_saving=True,
        is_distributed=False,
        rank=0,
        world_size=1
    )
    
    step = 0
    total_samples = 0
    start_time = time.time()
    
    data_iter = iter(train_loader)
    
    while step < num_steps:
        try:
            data, target = next(data_iter)
        except StopIteration:
            data_iter = iter(train_loader)
            data, target = next(data_iter)
        
        data, target = data.to(device), target.to(device)
        batch_size = data.size(0)
        
        # è®­ç»ƒæ­¥
        step_start = time.time()
        
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output.view(-1, output.size(-1)), target.view(-1))
        loss.backward()
        optimizer.step()
        
        step_time = (time.time() - step_start) * 1000
        metrics.add_step_time(step_time)
        
        total_samples += batch_size
        step += 1
        
        # æ£€æŸ¥ç‚¹ä¿å­˜
        if step % checkpoint_freq == 0:
            chk_start = time.time()
            
            # æ›´æ–° checkpoint_dict ä¸­çš„çŠ¶æ€
            monitor.checkpoint_dict['model'] = model.state_dict()
            monitor.checkpoint_dict['optimizer'] = optimizer.state_dict()
            
            # ä½¿ç”¨ PCCheck Monitor ä¿å­˜
            monitor.save()
            
            chk_time = (time.time() - chk_start) * 1000
            metrics.add_checkpoint_time(chk_time)
            # å‡å°‘è¾“å‡ºé¢‘ç‡
            if step % (checkpoint_freq * 5) == 0:  # æ¯5æ¬¡æ£€æŸ¥ç‚¹è¾“å‡ºä¸€æ¬¡
                print(f"  âœ“ Step {step}: PCCheck ä¿å­˜ ({chk_time:.2f} ms)")
        
        # è®°å½•å†…å­˜
        if step % 10 == 0:
            metrics.record_memory()
    
    total_time = time.time() - start_time
    metrics.total_time = total_time
    metrics.num_samples = total_samples
    metrics.throughput = total_samples / total_time
    
    # å…³é—­ monitor
    monitor.kill_checkpoint()
    
    return metrics


def benchmark_layerwise_pccheck(
    model: nn.Module,
    train_loader: DataLoader,
    criterion,
    device: str,
    checkpoint_dir: str,
    num_threads: int,
    max_async: int,
    buffer_size_mb: float,
    batch_size_mb: float,
    use_monitor: bool,
    checkpoint_freq: int,
    num_steps: int = 100
) -> BenchmarkMetrics:
    """æµ‹è¯•æ”¹è¿›çš„åˆ†å±‚ PCCheck"""
    
    print(f"\n{'='*80}")
    print(f"ğŸŸ£ å¼€å§‹æµ‹è¯•: æ”¹è¿›çš„åˆ†å±‚ PCCheck")
    print(f"{'='*80}")
    
    metrics = BenchmarkMetrics("Layerwise PCCheck (Improved)")
    
    # åˆ›å»ºåˆ†å±‚æ£€æŸ¥ç‚¹è®­ç»ƒå™¨
    print(f"ğŸ“ åˆå§‹åŒ–åˆ†å±‚è®­ç»ƒå™¨:")
    print(f"   - Threads: {num_threads}")
    print(f"   - Max async: {max_async}")
    print(f"   - Buffer size: {buffer_size_mb} MB")
    print(f"   - Batch size: {batch_size_mb} MB")
    print(f"   - Use monitor: {use_monitor}")
    
    c_lib_path = "/home/linzhicheng/code/pccheck/checkpoint_eval/pccheck/libtest_ssd.so"
    
    # ä½¿ç”¨pccheckçš„initåˆ›å»ºgpuç©ºé—´
    trainer = LayerwiseCheckpointTrainer(
        model=model,
        optimizer_class=torch.optim.Adam,
        optimizer_kwargs={'lr': 0.001},
        checkpoint_dir=checkpoint_dir,
        buffer_size_mb=buffer_size_mb,
        use_pccheck=True,
        use_monitor=use_monitor,
        num_threads=num_threads,
        max_async=max_async,
        batch_size_mb=batch_size_mb,
        ratio=2.0,
        c_lib_path=c_lib_path,
        device=device,
        verbose=False
    )
    
    model.train()
    
    step = 0
    total_samples = 0
    start_time = time.time()
    
    data_iter = iter(train_loader)
    
    # ç”¨äºæµ‹é‡æ£€æŸ¥ç‚¹æ—¶é—´çš„å˜é‡
    last_checkpoint_step = 0
    checkpoint_start_time = None
    
    while step < num_steps:
        try:
            data, target = next(data_iter)
        except StopIteration:
            data_iter = iter(train_loader)
            data, target = next(data_iter)
        
        data, target = data.to(device), target.to(device)
        batch_size = data.size(0)
        
        # ğŸ”¥ åˆ¤æ–­æ˜¯å¦éœ€è¦ä¿å­˜æ£€æŸ¥ç‚¹
        need_checkpoint = (step % checkpoint_freq == 0 and step > 0)
        
        # è®­ç»ƒæ­¥ (åŒ…å«åˆ†å±‚æ£€æŸ¥ç‚¹ä¿å­˜)
        step_start = time.time()
        
        # æ£€æŸ¥ç‚¹ä¿å­˜å¼€å§‹æ—¶åˆ»
        if need_checkpoint:
            checkpoint_start_time = time.time()
            last_checkpoint_step = step
        
        # ğŸ”¥ ä½¿ç”¨ trainer çš„ train_stepï¼Œåªåœ¨éœ€è¦æ—¶å¯ç”¨æ£€æŸ¥ç‚¹å›è°ƒ
        loss_value = trainer.train_step(
            data, target, criterion, 
            enable_checkpoint=need_checkpoint  # ğŸ”¥ å…³é”®ï¼šæ§åˆ¶æ˜¯å¦è§¦å‘å›è°ƒ
        )
        
        # å¦‚æœæ˜¯æ£€æŸ¥ç‚¹æ­¥éª¤ï¼Œå®Œæˆæ£€æŸ¥ç‚¹
        if need_checkpoint:
            trainer.finalize_checkpoint()
            chk_time = (time.time() - checkpoint_start_time) * 1000
            metrics.add_checkpoint_time(chk_time)
            # å‡å°‘è¾“å‡ºé¢‘ç‡ï¼Œåªåœ¨æŸäº›æ­¥éª¤è¾“å‡º
            if step % (checkpoint_freq * 5) == 0:  # æ¯5æ¬¡æ£€æŸ¥ç‚¹è¾“å‡ºä¸€æ¬¡
                print(f"  âœ“ Step {step}: åˆ†å±‚ä¿å­˜å®Œæˆ ({chk_time:.2f} ms)")
        
        step_time = (time.time() - step_start) * 1000
        metrics.add_step_time(step_time)
        
        total_samples += batch_size
        step += 1
        
        # è®°å½•å†…å­˜
        if step % 10 == 0:
            metrics.record_memory()
    
    total_time = time.time() - start_time
    metrics.total_time = total_time
    metrics.num_samples = total_samples
    metrics.throughput = total_samples / total_time
    
    # å…³é—­è®­ç»ƒå™¨
    trainer.shutdown()
    
    return metrics


def compare_methods(results: Dict[str, BenchmarkMetrics], output_file: str):
    """å¯¹æ¯”ä¸åŒæ–¹æ³•çš„ç»“æœ"""
    
    print(f"\n{'='*80}")
    print(f"ğŸ“Š å®éªŒç»“æœå¯¹æ¯”")
    print(f"{'='*80}\n")
    
    # æ”¶é›†ç»Ÿè®¡æ•°æ®
    all_stats = {}
    for name, metrics in results.items():
        all_stats[name] = metrics.compute_statistics()
    
    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    print(f"{'æŒ‡æ ‡':<30} {'ä¼ ç»Ÿ':<20} {'åŸå§‹PCCheck':<20} {'åˆ†å±‚PCCheck':<20}")
    print(f"{'-'*90}")
    
    # åŸºå‡†æ–¹æ³•
    baseline_name = "Traditional PyTorch Checkpoint"
    baseline = all_stats.get(baseline_name)
    
    # ååé‡å¯¹æ¯”
    print(f"\nğŸš€ ååé‡ (samples/sec):")
    for name, stats in all_stats.items():
        throughput = stats['throughput']['samples_per_sec']
        if baseline and name != baseline_name:
            speedup = throughput / baseline['throughput']['samples_per_sec']
            print(f"  {name:<30}: {throughput:>10.2f}  (speedup: {speedup:.2f}x)")
        else:
            print(f"  {name:<30}: {throughput:>10.2f}  (baseline)")
    
    # æ£€æŸ¥ç‚¹å¼€é”€å¯¹æ¯”
    print(f"\nğŸ’¾ æ£€æŸ¥ç‚¹å¼€é”€:")
    for name, stats in all_stats.items():
        overhead = stats['checkpoint_overhead_percent']
        mean_time = stats['checkpoint']['mean_ms']
        if baseline and name != baseline_name:
            reduction = (1 - overhead / baseline['checkpoint_overhead_percent']) * 100
            print(f"  {name:<30}: {overhead:>6.2f}%  (å¹³å‡ {mean_time:>7.2f}ms, é™ä½ {reduction:>5.1f}%)")
        else:
            print(f"  {name:<30}: {overhead:>6.2f}%  (å¹³å‡ {mean_time:>7.2f}ms, baseline)")
    
    # å†…å­˜ä½¿ç”¨å¯¹æ¯”
    print(f"\nğŸ’» å³°å€¼å†…å­˜ (GB):")
    for name, stats in all_stats.items():
        cpu_mem = stats['memory']['peak_cpu_gb']
        gpu_mem = stats['memory']['peak_gpu_gb']
        print(f"  {name:<30}: CPU {cpu_mem:>6.2f}, GPU {gpu_mem:>6.2f}")
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    output_data = {
        'timestamp': datetime.now().isoformat(),
        'results': all_stats
    }
    
    with open(output_file, 'w') as f:
        json.dump(output_data, f, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print(f"{'='*80}\n")


def main():
    parser = argparse.ArgumentParser(description='PCCheck æ”¹è¿›æ•ˆæœå¯¹æ¯”å®éªŒ')
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--vocab-size', type=int, default=10000, help='è¯æ±‡è¡¨å¤§å°')
    parser.add_argument('--d-model', type=int, default=512, help='æ¨¡å‹ç»´åº¦')
    parser.add_argument('--nhead', type=int, default=8, help='æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--num-layers', type=int, default=6, help='Transformer å±‚æ•°')
    parser.add_argument('--dim-feedforward', type=int, default=2048, help='å‰é¦ˆç½‘ç»œç»´åº¦')
    
    # è®­ç»ƒé…ç½®
    parser.add_argument('--num-samples', type=int, default=1000, help='è®­ç»ƒæ ·æœ¬æ•°')
    parser.add_argument('--seq-len', type=int, default=128, help='åºåˆ—é•¿åº¦')
    parser.add_argument('--batch-size', type=int, default=16, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--num-steps', type=int, default=100, help='è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--checkpoint-freq', type=int, default=10, help='æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡')
    
    # PCCheck é…ç½®
    parser.add_argument('--num-threads', type=int, default=8, help='PCCheck çº¿ç¨‹æ•°')
    parser.add_argument('--max-async', type=int, default=4, help='æœ€å¤§å¹¶å‘æ£€æŸ¥ç‚¹æ•°')
    parser.add_argument('--buffer-size-mb', type=float, default=50.0, help='ç¼“å†²åŒºå¤§å° (MB)')
    parser.add_argument('--batch-size-mb', type=float, default=100.0, help='PCCheck æ‰¹æ¬¡å¤§å° (MB)')
    parser.add_argument('--use-monitor', action='store_true', help='ä½¿ç”¨ Monitor æ¨¡å¼')
    
    # å®éªŒé…ç½®
    parser.add_argument('--methods', nargs='+', default=['traditional', 'original', 'layerwise'],
                        choices=['traditional', 'original', 'layerwise'],
                        help='è¦æµ‹è¯•çš„æ–¹æ³•')
    parser.add_argument('--output-dir', type=str, default='./benchmark_results',
                        help='ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu',
                        help='è®­ç»ƒè®¾å¤‡')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    print(f"\n{'='*80}")
    print(f"ğŸ”¬ PCCheck æ”¹è¿›æ•ˆæœå¯¹æ¯”å®éªŒ")
    print(f"{'='*80}")
    print(f"ğŸ“… æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {args.device}")
    print(f"ğŸ“ é…ç½®:")
    print(f"   - æ¨¡å‹: Transformer (d={args.d_model}, layers={args.num_layers})")
    print(f"   - æ•°æ®: {args.num_samples} samples, seq_len={args.seq_len}")
    print(f"   - è®­ç»ƒ: {args.num_steps} steps, batch_size={args.batch_size}")
    print(f"   - æ£€æŸ¥ç‚¹é¢‘ç‡: æ¯ {args.checkpoint_freq} æ­¥")
    print(f"   - æµ‹è¯•æ–¹æ³•: {', '.join(args.methods)}")
    print(f"{'='*80}\n")
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = create_synthetic_dataset(
        num_samples=args.num_samples,
        seq_len=args.seq_len,
        vocab_size=args.vocab_size
    )
    train_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)
    
    # æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()
    
    # å­˜å‚¨ç»“æœ
    results = {}
    
    # æµ‹è¯•ä¼ ç»Ÿæ–¹æ³•
    if 'traditional' in args.methods:
        model = TestModel(
            vocab_size=args.vocab_size,
            d_model=args.d_model,
            nhead=args.nhead,
            num_layers=args.num_layers,
            dim_feedforward=args.dim_feedforward
        ).to(args.device)
        
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        
        checkpoint_dir = os.path.join(args.output_dir, f'traditional_{timestamp}')
        
        metrics = benchmark_traditional_checkpoint(
            model, train_loader, criterion, optimizer,
            args.device, args.checkpoint_freq, checkpoint_dir, args.num_steps
        )
        metrics.print_summary()
        results['Traditional PyTorch Checkpoint'] = metrics
        
        # æ¸…ç†
        del model, optimizer
        torch.cuda.empty_cache()
    
    # æµ‹è¯•åŸå§‹ PCCheck
    if 'original' in args.methods:
        model = TestModel(
            vocab_size=args.vocab_size,
            d_model=args.d_model,
            nhead=args.nhead,
            num_layers=args.num_layers,
            dim_feedforward=args.dim_feedforward
        ).to(args.device)
        
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        
        checkpoint_file = os.path.join(args.output_dir, f'original_{timestamp}.chk')
        
        metrics = benchmark_original_pccheck(
            model, train_loader, criterion, optimizer,
            args.device, args.checkpoint_freq, checkpoint_file,
            args.num_threads, args.max_async, args.num_steps
        )
        metrics.print_summary()
        results['Original PCCheck'] = metrics
        
        # æ¸…ç†
        del model, optimizer
        torch.cuda.empty_cache()
    
    # æµ‹è¯•åˆ†å±‚ PCCheck
    if 'layerwise' in args.methods:
        model = TestModel(
            vocab_size=args.vocab_size,
            d_model=args.d_model,
            nhead=args.nhead,
            num_layers=args.num_layers,
            dim_feedforward=args.dim_feedforward
        ).to(args.device)
        
        checkpoint_dir = os.path.join(args.output_dir, f'layerwise_{timestamp}')
        
        metrics = benchmark_layerwise_pccheck(
            model, train_loader, criterion,
            args.device, checkpoint_dir,
            args.num_threads, args.max_async,
            args.buffer_size_mb, args.batch_size_mb,
            True, args.checkpoint_freq, args.num_steps
        )
        metrics.print_summary()
        results['Layerwise PCCheck (Improved)'] = metrics
        
        # æ¸…ç†
        del model
        torch.cuda.empty_cache()
    
    # å¯¹æ¯”ç»“æœ
    if len(results) > 1:
        output_file = os.path.join(args.output_dir, f'comparison_{timestamp}.json')
        compare_methods(results, output_file)
    
    print(f"\nâœ… å®éªŒå®Œæˆï¼")


if __name__ == "__main__":
    main()
