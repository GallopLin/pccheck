"""
é˜¶æ®µäºŒï¼šæŒ‚é’©ï¼ˆHookï¼‰ä¼˜åŒ–å™¨ä¸è®­ç»ƒå¾ªç¯
Layerwise Optimizer Wrapper

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç²¾ç¡®æ•è·æ¯ä¸ªå±‚å‚æ•°æ›´æ–°å®Œæˆçš„äº‹ä»¶ï¼Œå¹¶è§¦å‘å›è°ƒ
"""

import torch
import torch.nn as nn
from torch.optim import Optimizer
from typing import Callable, Dict, List, Optional, Any
from collections import OrderedDict
import time
import copy


class LayerwiseOptimizer:
    """
    åˆ†å±‚ä¼˜åŒ–å™¨åŒ…è£…å™¨
    
    åŒ…è£…æ ‡å‡†çš„ PyTorch ä¼˜åŒ–å™¨ï¼Œåœ¨æ¯å±‚å‚æ•°æ›´æ–°åè§¦å‘å›è°ƒå‡½æ•°ã€‚
    è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥åœ¨å‚æ•°æ›´æ–°å®Œæˆåç«‹å³å¼€å§‹ä¿å­˜è¯¥å±‚ï¼Œè€Œä¸éœ€è¦ç­‰å¾…æ‰€æœ‰å±‚éƒ½æ›´æ–°å®Œæˆã€‚
    """
    
    def __init__(
        self,
        optimizer: Optimizer,
        model: nn.Module,
        update_order: List[str],
        layer_info: Dict[str, Dict],
        callback: Optional[Callable[[str, int, Dict], None]] = None,
        enable_timing: bool = False,
        verbose: bool = False
    ):
        """
        Args:
            optimizer: PyTorch ä¼˜åŒ–å™¨å®ä¾‹ (Adam, SGD, etc.)
            model: PyTorch æ¨¡å‹å®ä¾‹
            update_order: å±‚æ›´æ–°é¡ºåºåˆ—è¡¨ï¼ˆä» DependencyGraphBuilder è·å–ï¼‰
            layer_info: å±‚ä¿¡æ¯å­—å…¸ï¼ˆä» DependencyGraphBuilder è·å–ï¼‰
            callback: æ¯å±‚æ›´æ–°åçš„å›è°ƒå‡½æ•°ï¼Œç­¾åä¸º callback(layer_name, step, layer_params)
            enable_timing: æ˜¯å¦å¯ç”¨æ€§èƒ½è®¡æ—¶
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        """
        self.optimizer = optimizer
        self.model = model
        self.update_order = update_order
        self.layer_info = layer_info
        self.callback = callback
        self.enable_timing = enable_timing
        self.verbose = verbose
        
        # è®­ç»ƒæ­¥æ•°è®¡æ•°å™¨
        self.training_step = 0
        
        # ğŸ”¥ æ–°å¢ï¼šæ£€æŸ¥ç‚¹æ§åˆ¶æœºåˆ¶
        self._checkpoint_enabled = False
        self._checkpoint_mode = 'manual'  # 'auto', 'manual', 'disabled'
        
        # æ€§èƒ½ç»Ÿè®¡
        self.timing_stats = {
            'total_step_time': [],
            'update_time_per_layer': {layer: [] for layer in update_order},
            'callback_time_per_layer': {layer: [] for layer in update_order}
        }
        
        # æ„å»ºå±‚åç§°åˆ°å‚æ•°çš„æ˜ å°„
        self._build_layer_param_mapping()
        
        # æ„å»ºå±‚åç§°åˆ°ä¼˜åŒ–å™¨ param_groups çš„æ˜ å°„
        self._build_layer_param_groups()
        
    def _build_layer_param_mapping(self):
        """æ„å»ºå±‚åç§°åˆ°å‚æ•°å¼ é‡çš„æ˜ å°„"""
        self.layer_to_params = OrderedDict()
        
        for layer_name in self.update_order:
            if layer_name in self.layer_info:
                params = self.layer_info[layer_name]['parameters']
                # å­˜å‚¨å‚æ•°çš„å¼•ç”¨
                self.layer_to_params[layer_name] = params
            else:
                raise KeyError(f"Layer '{layer_name}' not found in layer_info")
        
        if self.verbose:
            print(f"æ„å»ºäº† {len(self.layer_to_params)} ä¸ªå±‚çš„å‚æ•°æ˜ å°„")
    
    def _build_layer_param_groups(self):
        """
        ä¸ºæ¯ä¸ªå±‚åˆ›å»ºç‹¬ç«‹çš„ param_groups
        è¿™æ ·å¯ä»¥å•ç‹¬æ›´æ–°æ¯ä¸ªå±‚çš„å‚æ•°
        """
        self.layer_param_groups = OrderedDict()
        
        # è·å–ä¼˜åŒ–å™¨çš„æ‰€æœ‰å‚æ•°
        all_optimizer_params = set()
        for group in self.optimizer.param_groups:
            all_optimizer_params.update(id(p) for p in group['params'])
        
        # ä¸ºæ¯ä¸ªå±‚åˆ›å»º param_group
        for layer_name, params in self.layer_to_params.items():
            # è¿‡æ»¤å‡ºå±äºè¯¥å±‚ä¸”åœ¨ä¼˜åŒ–å™¨ä¸­çš„å‚æ•°
            layer_params_in_optimizer = [
                p for p in params 
                if id(p) in all_optimizer_params and p.requires_grad
            ]
            
            if layer_params_in_optimizer:
                # å¤åˆ¶åŸå§‹ param_group çš„é…ç½®ï¼ˆlr, weight_decay ç­‰ï¼‰
                # è¿™é‡Œå‡è®¾æ‰€æœ‰å‚æ•°ä½¿ç”¨ç›¸åŒçš„é…ç½®
                base_config = {
                    k: v for k, v in self.optimizer.param_groups[0].items()
                    if k != 'params'
                }
                
                self.layer_param_groups[layer_name] = {
                    'params': layer_params_in_optimizer,
                    **base_config
                }
        
        if self.verbose:
            print(f"ä¸º {len(self.layer_param_groups)} ä¸ªå±‚åˆ›å»ºäº†ç‹¬ç«‹çš„ param_groups")
    
    def enable_checkpointing(self, enable: bool = True):
        """
        å¯ç”¨æˆ–ç¦ç”¨æ£€æŸ¥ç‚¹å›è°ƒ
        
        Args:
            enable: True=å¯ç”¨æ£€æŸ¥ç‚¹å›è°ƒ, False=ç¦ç”¨ï¼ˆæ­£å¸¸è®­ç»ƒï¼Œä¸è§¦å‘å›è°ƒï¼‰
        """
        self._checkpoint_enabled = enable
    
    def set_checkpoint_mode(self, mode: str):
        """
        è®¾ç½®æ£€æŸ¥ç‚¹æ¨¡å¼
        
        Args:
            mode: 'auto' (æ¯æ­¥è‡ªåŠ¨è§¦å‘å›è°ƒ), 
                  'manual' (æ‰‹åŠ¨æ§åˆ¶ï¼Œé»˜è®¤), 
                  'disabled' (å®Œå…¨ç¦ç”¨)
        """
        assert mode in ['auto', 'manual', 'disabled'], \
            f"Invalid mode: {mode}. Must be 'auto', 'manual', or 'disabled'"
        
        self._checkpoint_mode = mode
        
        # æ ¹æ®æ¨¡å¼è®¾ç½®é»˜è®¤çŠ¶æ€
        if mode == 'auto':
            self._checkpoint_enabled = True
        elif mode == 'disabled':
            self._checkpoint_enabled = False
    
    def _should_trigger_callback(self) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥è§¦å‘å›è°ƒ
        
        Returns:
            bool: True=åº”è¯¥è§¦å‘å›è°ƒ, False=è·³è¿‡å›è°ƒ
        """
        # æ²¡æœ‰å›è°ƒå‡½æ•°ï¼Œç›´æ¥è¿”å› False
        if self.callback is None:
            return False
        
        # å®Œå…¨ç¦ç”¨æ¨¡å¼
        if self._checkpoint_mode == 'disabled':
            return False
        
        # è‡ªåŠ¨æ¨¡å¼ï¼ˆæ¯æ­¥éƒ½è§¦å‘ï¼‰
        if self._checkpoint_mode == 'auto':
            return True
        
        # æ‰‹åŠ¨æ¨¡å¼ï¼ˆæ ¹æ® enable_checkpointing è®¾ç½®ï¼‰
        if self._checkpoint_mode == 'manual':
            return self._checkpoint_enabled
        
        return False
    
    def zero_grad(self, set_to_none: bool = False):
        """
        æ¸…é›¶æ¢¯åº¦ï¼ˆä»£ç†åˆ°åº•å±‚ä¼˜åŒ–å™¨ï¼‰
        """
        self.optimizer.zero_grad(set_to_none=set_to_none)
    
    def step(self, closure: Optional[Callable] = None):
        """
        æ‰§è¡Œä¸€æ­¥ä¼˜åŒ–ï¼ˆå…³é”®æ–¹æ³•ï¼‰
        
        ğŸ”¥ ä¼˜åŒ–ï¼šæ·»åŠ å¿«é€Ÿè·¯å¾„
        - å¦‚æœä¸éœ€è¦æ£€æŸ¥ç‚¹å›è°ƒï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ä¼˜åŒ–å™¨çš„ step()ï¼Œé¿å…åˆ†å±‚æ›´æ–°å¼€é”€
        - å¦‚æœéœ€è¦æ£€æŸ¥ç‚¹ï¼Œæ‰æŒ‰ç…§æ›´æ–°é¡ºåºé€å±‚æ›´æ–°å‚æ•°å¹¶è§¦å‘å›è°ƒ
        
        Args:
            closure: å¯é€‰çš„é—­åŒ…å‡½æ•°ï¼ˆæŸäº›ä¼˜åŒ–å™¨å¦‚ LBFGS éœ€è¦ï¼‰
        """
        step_start_time = time.time() if self.enable_timing else None
        
        self.training_step += 1
        
        # ğŸ”¥ å¿«é€Ÿè·¯å¾„ï¼šå¦‚æœä¸éœ€è¦è§¦å‘æ£€æŸ¥ç‚¹å›è°ƒï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ä¼˜åŒ–å™¨
        if not self._should_trigger_callback():
            if self.verbose:
                print(f"\n[æ­¥éª¤ {self.training_step}] ä½¿ç”¨å¿«é€Ÿè·¯å¾„ï¼ˆæ— æ£€æŸ¥ç‚¹ï¼‰")
            
            # ç›´æ¥è°ƒç”¨åŸå§‹ä¼˜åŒ–å™¨çš„ stepï¼Œä¸€æ¬¡æ€§æ›´æ–°æ‰€æœ‰å‚æ•°
            self.optimizer.step(closure)
            
            if self.enable_timing:
                total_step_time = time.time() - step_start_time
                self.timing_stats['total_step_time'].append(total_step_time)
                if self.verbose:
                    print(f"å¿«é€Ÿè·¯å¾„è€—æ—¶: {total_step_time*1000:.2f} ms")
            
            return
        
        # ğŸ”¥ æ…¢é€Ÿè·¯å¾„ï¼šéœ€è¦æ£€æŸ¥ç‚¹ï¼Œæ‰§è¡Œåˆ†å±‚æ›´æ–°
        if self.verbose:
            print(f"\n{'='*80}")
            print(f"å¼€å§‹ç¬¬ {self.training_step} æ­¥ä¼˜åŒ–ï¼ˆåˆ†å±‚æ›´æ–°æ¨¡å¼ + æ£€æŸ¥ç‚¹ï¼‰")
            print(f"{'='*80}")
        
        # æŒ‰ç…§ä¾èµ–é¡ºåºé€å±‚æ›´æ–°å‚æ•°
        for layer_idx, layer_name in enumerate(self.update_order):
            if layer_name not in self.layer_param_groups:
                continue
            
            layer_update_start = time.time() if self.enable_timing else None
            
            # è·å–è¯¥å±‚çš„å‚æ•°
            layer_param_group = self.layer_param_groups[layer_name]
            
            # æ‰§è¡Œè¯¥å±‚çš„å‚æ•°æ›´æ–°
            # è¿™é‡Œæˆ‘ä»¬æ‰‹åŠ¨è°ƒç”¨ä¼˜åŒ–å™¨çš„æ›´æ–°é€»è¾‘
            self._update_layer_params(layer_param_group, closure)
            
            if self.enable_timing:
                update_time = time.time() - layer_update_start
                self.timing_stats['update_time_per_layer'][layer_name].append(update_time)
            
            if self.verbose:
                param_count = sum(p.numel() for p in layer_param_group['params'])
                print(f"  [{layer_idx+1:2d}/{len(self.update_order):2d}] "
                      f"æ›´æ–° {layer_name:40s} | {param_count:12,d} å‚æ•°", end='')
            
            # è§¦å‘æ£€æŸ¥ç‚¹å›è°ƒ
            callback_start = time.time() if self.enable_timing else None
            
            # å‡†å¤‡å›è°ƒæ‰€éœ€çš„å‚æ•°ä¿¡æ¯
            layer_params_dict = self._prepare_layer_params_for_callback(layer_name)
            
            # è°ƒç”¨å›è°ƒå‡½æ•°
            self.callback(layer_name, self.training_step, layer_params_dict)
            
            if self.enable_timing:
                callback_time = time.time() - callback_start
                self.timing_stats['callback_time_per_layer'][layer_name].append(callback_time)
                
                if self.verbose:
                    print(f" | å›è°ƒè€—æ—¶: {callback_time*1000:.2f} ms")
            elif self.verbose:
                print()
        
        if self.enable_timing:
            total_step_time = time.time() - step_start_time
            self.timing_stats['total_step_time'].append(total_step_time)
            
            if self.verbose:
                print(f"\næ€»æ­¥éª¤è€—æ—¶: {total_step_time*1000:.2f} ms")
        
        if self.verbose:
            print(f"{'='*80}\n")
    
    def _update_layer_params(self, param_group: Dict, closure: Optional[Callable] = None):
        """
        æ›´æ–°å•ä¸ªå±‚çš„å‚æ•°
        
        è¿™ä¸ªæ–¹æ³•æ¨¡æ‹Ÿä¼˜åŒ–å™¨çš„å•æ­¥æ›´æ–°ï¼Œä½†åªé’ˆå¯¹æŒ‡å®šçš„å‚æ•°ç»„
        """
        # ä¸´æ—¶æ›¿æ¢ä¼˜åŒ–å™¨çš„ param_groupsï¼ŒåªåŒ…å«å½“å‰å±‚
        original_param_groups = self.optimizer.param_groups
        self.optimizer.param_groups = [param_group]
        
        # æ‰§è¡Œä¼˜åŒ–å™¨çš„æ­¥è¿›
        # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥è°ƒç”¨ä¼˜åŒ–å™¨çš„ stepï¼Œå®ƒä¼šæ›´æ–° param_groups ä¸­çš„å‚æ•°
        self.optimizer.step(closure)
        
        # æ¢å¤åŸå§‹çš„ param_groups
        self.optimizer.param_groups = original_param_groups
    
    def _prepare_layer_params_for_callback(self, layer_name: str) -> Dict[str, Any]:
        """
        ä¸ºå›è°ƒå‡½æ•°å‡†å¤‡è¯¥å±‚çš„å‚æ•°ä¿¡æ¯
        
        ğŸ”¥ ä¼˜åŒ–ï¼šä¸å†è¿›è¡Œæ·±æ‹·è´ï¼Œåªä¼ é€’å¼•ç”¨
        æ·±æ‹·è´æ”¹ä¸ºåœ¨ PCCheckAdapter ä¸­å®Œæˆï¼Œç›´æ¥ä»åŸåœ°å€æ‹·è´åˆ° staging buffer
        è¿™æ ·å¯ä»¥é¿å…ä¸€æ¬¡é¢å¤–çš„ GPU å†…å­˜æ‹·è´
        
        Returns:
            åŒ…å«å±‚å‚æ•°çš„å­—å…¸ï¼ŒåŒ…æ‹¬å¼ é‡çš„å¼•ç”¨ï¼ˆä¸å†æ·±æ‹·è´ï¼‰
        """
        params = self.layer_to_params[layer_name]
        
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šä¸å†æ·±æ‹·è´ï¼Œåªä¼ é€’å¼•ç”¨
        # params_copy = [p.detach().clone() for p in params]  # æ—§ç‰ˆæœ¬
        params_ref = [p for p in params]  # æ–°ç‰ˆæœ¬ï¼šåªä¼ é€’å¼•ç”¨
        
        return {
            'layer_name': layer_name,
            'parameters': params_ref,  # ä¼ é€’å¼•ç”¨è€Œä¸æ˜¯æ‹·è´
            'param_count': sum(p.numel() for p in params_ref),
            'shapes': [p.shape for p in params_ref],
            'dtypes': [p.dtype for p in params_ref],
            'devices': [p.device for p in params_ref],
            'training_step': self.training_step
        }
    
    def state_dict(self):
        """è¿”å›ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆä»£ç†åˆ°åº•å±‚ä¼˜åŒ–å™¨ï¼‰"""
        return self.optimizer.state_dict()
    
    def load_state_dict(self, state_dict):
        """åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆä»£ç†åˆ°åº•å±‚ä¼˜åŒ–å™¨ï¼‰"""
        self.optimizer.load_state_dict(state_dict)
    
    def get_timing_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        if not self.enable_timing:
            return {"message": "Timing is not enabled"}
        
        import numpy as np
        
        stats = {
            'total_steps': self.training_step,
            'avg_step_time_ms': np.mean(self.timing_stats['total_step_time']) * 1000,
            'layer_stats': {}
        }
        
        for layer_name in self.update_order:
            update_times = self.timing_stats['update_time_per_layer'][layer_name]
            callback_times = self.timing_stats['callback_time_per_layer'][layer_name]
            
            if update_times:
                stats['layer_stats'][layer_name] = {
                    'avg_update_time_ms': np.mean(update_times) * 1000,
                    'avg_callback_time_ms': np.mean(callback_times) * 1000 if callback_times else 0,
                    'total_time_ms': (np.mean(update_times) + np.mean(callback_times or [0])) * 1000
                }
        
        return stats
    
    def print_timing_stats(self):
        """æ‰“å°æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_timing_stats()
        
        if 'message' in stats:
            print(stats['message'])
            return
        
        print(f"\n{'='*80}")
        print(f"æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š (åŸºäº {stats['total_steps']} æ­¥)")
        print(f"{'='*80}")
        print(f"å¹³å‡æ¯æ­¥æ€»è€—æ—¶: {stats['avg_step_time_ms']:.2f} ms\n")
        
        print(f"{'å±‚åç§°':<40s} | {'æ›´æ–°è€—æ—¶':<12s} | {'å›è°ƒè€—æ—¶':<12s} | {'æ€»è€—æ—¶':<12s}")
        print(f"{'-'*80}")
        
        for layer_name, layer_stats in stats['layer_stats'].items():
            print(f"{layer_name:<40s} | "
                  f"{layer_stats['avg_update_time_ms']:>10.2f} ms | "
                  f"{layer_stats['avg_callback_time_ms']:>10.2f} ms | "
                  f"{layer_stats['total_time_ms']:>10.2f} ms")
        
        print(f"{'='*80}\n")
