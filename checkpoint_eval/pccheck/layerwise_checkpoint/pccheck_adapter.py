"""
é˜¶æ®µå››ï¼šPCCheck åç«¯é€‚é…å™¨
PCCheck Backend Adapter for Layerwise Checkpointing

å°†åˆ†å±‚æ£€æŸ¥ç‚¹ä»»åŠ¡é€‚é…åˆ° PCCheck çš„æµæ°´çº¿ç³»ç»Ÿ
"""

import torch
import numpy as np
import time
import threading
import os
from typing import List, Dict, Optional
from dataclasses import dataclass
import json

# å¯¼å…¥ PCCheck åŸå§‹ç»„ä»¶
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from chk_checkpoint_pipeline import Checkpoint, Writer
    from chk_monitor import Chk_monitor
    PCCHECK_AVAILABLE = True
except ImportError:
    print("[Warning] PCCheck åç«¯ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
    PCCHECK_AVAILABLE = False
    Checkpoint = None
    Chk_monitor = None
    Writer = None


@dataclass
class LayerMetadata:
    """å±‚çš„å…ƒæ•°æ®"""
    layer_name: str
    training_step: int
    checkpoint_id: str
    offset_in_file: int  # åœ¨æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸­çš„åç§»é‡
    size_bytes: int
    param_count: int
    shapes: List[tuple]
    dtypes: List[str]
    timestamp: float


class PCCheckAdapter:
    """
    PCCheck åç«¯é€‚é…å™¨
    
    è´Ÿè´£å°†åˆ†å±‚ä¿å­˜ä»»åŠ¡è½¬æ¢ä¸º PCCheck èƒ½å¤Ÿå¤„ç†çš„æ ¼å¼
    """
    
    def __init__(
        self,
        c_lib_path: str,
        checkpoint_file: str = "layerwise_checkpoint.chk",
        num_threads: int = 4,
        max_async: int = 2,
        batch_size_mb: float = 100.0,
        ratio: float = 2.0,
        use_pccheck: bool = True,
        use_monitor: bool = False,  # æ˜¯å¦ä½¿ç”¨ Chk_monitorï¼ˆåå°è¿›ç¨‹æ¨¡å¼ï¼‰
        metadata_file: str = "checkpoint_metadata.json",
        is_distributed: bool = False,
        rank: int = 0,
        world_size: int = 1,
        gpu_ar: Optional[torch.Tensor] = None,  # ğŸ”¥ æ–°å¢ï¼šå¤–éƒ¨ä¼ å…¥çš„ gpu_ar
        total_size: int = 0,  # ğŸ”¥ æ–°å¢ï¼šæ€»å¤§å°ï¼ˆå…ƒç´ æ•°ï¼‰
        verbose: bool = False
    ):
        """
        Args:
            c_lib_path: PCCheck C åº“è·¯å¾„
            checkpoint_file: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
            num_threads: PCCheck ä½¿ç”¨çš„çº¿ç¨‹æ•°
            max_async: æœ€å¤§å¹¶å‘æ£€æŸ¥ç‚¹æ•°é‡
            batch_size_mb: æ¯ä¸ªæ‰¹æ¬¡çš„å¤§å°ï¼ˆMBï¼‰
            ratio: CPUç¼“å†²åŒºå¤§å°ç›¸å¯¹äºæ£€æŸ¥ç‚¹å¤§å°çš„å€æ•°
            use_pccheck: æ˜¯å¦ä½¿ç”¨çœŸå®çš„ PCCheckï¼ˆFalse åˆ™æ¨¡æ‹Ÿï¼‰
            use_monitor: æ˜¯å¦ä½¿ç”¨ Chk_monitorï¼ˆåå°è¿›ç¨‹æ¨¡å¼ï¼Œæ›´é«˜æ•ˆï¼‰
            metadata_file: å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
            is_distributed: æ˜¯å¦ä¸ºåˆ†å¸ƒå¼è®­ç»ƒ
            rank: å½“å‰è¿›ç¨‹çš„ rank
            world_size: æ€»è¿›ç¨‹æ•°
            gpu_ar: ğŸ”¥ å¤–éƒ¨ä¼ å…¥çš„ gpu staging bufferï¼ˆç”±åŸ PCCheck initialize æ„é€ ï¼‰
            total_size: ğŸ”¥ æ€»å¤§å°ï¼ˆfloat32 å…ƒç´ æ•°ï¼Œä¸ gpu_ar å¯¹åº”ï¼‰
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        """
        self.c_lib_path = c_lib_path
        self.checkpoint_file = checkpoint_file
        self.num_threads = num_threads
        self.max_async = max_async
        self.batch_size_mb = batch_size_mb
        self.ratio = ratio
        self.use_pccheck = use_pccheck and PCCHECK_AVAILABLE
        self.use_monitor = use_monitor and PCCHECK_AVAILABLE and (Chk_monitor is not None)
        self.metadata_file = metadata_file
        self.is_distributed = is_distributed
        self.rank = rank
        self.world_size = world_size
        self.verbose = verbose
        
        # ğŸ”¥ ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„ gpu_arï¼ˆç”±åŸ PCCheck initialize æ„é€ ï¼‰
        self.gpu_ar = gpu_ar  # ğŸ”¥ ç»Ÿä¸€ä½¿ç”¨ gpu_arï¼ˆä¸å†ä½¿ç”¨å†—ä½™çš„ staging_bufferï¼‰
        self.total_size_floats = total_size  # ä¿å­˜æ€»å¤§å°ï¼ˆfloat32 å…ƒç´ æ•°ï¼‰
        
        # CPU buffer (å¦‚æœä½¿ç”¨ PCCheck)
        self.cpu_buffer = None
        
        # å…ƒæ•°æ®ç®¡ç†
        self.layer_metadata = []  # List[LayerMetadata]
        self.current_file_offset = 0
        
        # PCCheck å®ä¾‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.pccheck_instance = None
        self.pccheck_monitor = None  # Chk_monitor å®ä¾‹
        self.checkpoint_lock = None
        self.cp_in_progress = None
        
        # æ‰¹æ¬¡ç®¡ç†
        self.batch_size_bytes = int(batch_size_mb * 1024 * 1024)
        self.batch_size_floats = self.batch_size_bytes // 4  # float32
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_layers_saved': 0,
            'total_bytes_saved': 0,
            'total_save_time': 0.0,
        }
        
        # åˆå§‹åŒ–
        self._initialize()
    
    def _initialize(self):
        """åˆå§‹åŒ–åç«¯"""
        if self.use_pccheck:
            self._initialize_pccheck()
        else:
            self._initialize_mock()
        
        if self.verbose:
            mode = "PCCheck" if self.use_pccheck else "Mock"
            print(f"[Adapter] åˆå§‹åŒ–å®Œæˆ (æ¨¡å¼: {mode})")
            print(f"  - æ£€æŸ¥ç‚¹æ–‡ä»¶: {self.checkpoint_file}")
            print(f"  - å…ƒæ•°æ®æ–‡ä»¶: {self.metadata_file}")
    
    def _initialize_pccheck(self):
        """åˆå§‹åŒ–çœŸå®çš„ PCCheck åç«¯"""
        try:
            from threading import Lock
            from multiprocessing import Value
            
            if self.verbose:
                print(f"[Adapter] åˆå§‹åŒ– PCCheck åç«¯...")
            
            # ğŸ”¥ ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„ total_sizeï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¼°ç®—
            if self.total_size_floats > 0:
                total_size_floats = self.total_size_floats
                estimated_total_size_mb = total_size_floats * 4 / (1024 * 1024)
                if self.verbose:
                    print(f"  - ä½¿ç”¨ä¼ å…¥çš„æ€»å¤§å°: {estimated_total_size_mb:.2f} MB ({total_size_floats:,} floats)")
            else:
                # ä¼°ç®—æ€»å¤§å°ï¼ˆè¿™é‡Œå…ˆè®¾ç½®ä¸€ä¸ªè¾ƒå¤§çš„å€¼ï¼Œå®é™…ä½¿ç”¨æ—¶å¯ä»¥æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´ï¼‰
                estimated_total_size_mb = 1000.0  # 1GB
                total_size_floats = int(estimated_total_size_mb * 1024 * 1024 / 4)  # float32
                if self.verbose:
                    print(f"  - ä¼°ç®—æ€»å¤§å°: {estimated_total_size_mb:.2f} MB ({total_size_floats:,} floats)")
            
            if self.verbose:
                print(f"  - æ‰¹æ¬¡å¤§å°: {self.batch_size_mb:.2f} MB ({self.batch_size_floats:,} floats)")
                print(f"  - çº¿ç¨‹æ•°: {self.num_threads}")
                print(f"  - æœ€å¤§å¼‚æ­¥æ•°: {self.max_async}")
                print(f"  - ç¼“å†²åŒºå€æ•°: {self.ratio}x")
                print(f"  - ä½¿ç”¨ Monitor: {self.use_monitor}")
            
            if self.use_monitor and Chk_monitor is not None:
                # ä½¿ç”¨ Chk_monitorï¼ˆåå°è¿›ç¨‹æ¨¡å¼ï¼Œæ›´é«˜æ•ˆï¼‰
                if self.verbose:
                    print(f"[Adapter] ä½¿ç”¨ Chk_monitor åå°è¿›ç¨‹æ¨¡å¼")
                
                # âš ï¸ å…³é”®ï¼šMonitor æ¨¡å¼å¿…é¡»ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„ gpu_ar
                # ç¡®ä¿ gpu_ar å·²ç»ç”±å¤–éƒ¨ initialize/set_storage è®¾ç½®å¥½
                if self.gpu_ar is None:
                    raise ValueError(
                        "Monitor æ¨¡å¼éœ€è¦å¤–éƒ¨ä¼ å…¥çš„ gpu_arï¼"
                        "è¯·åœ¨åˆ›å»º LayerwiseCheckpointTrainer æ—¶ç¡®ä¿ use_pccheck=Trueï¼Œ"
                        "ç³»ç»Ÿä¼šè‡ªåŠ¨è°ƒç”¨ initialize/set_storage æ¥æ„é€  gpu_ar"
                    )
                
                self.pccheck_monitor = Chk_monitor(
                    c_lib_path=self.c_lib_path,
                    total_size=total_size_floats,
                    num_threads=self.num_threads,
                    max_async=self.max_async,
                    gpu_copy=True,  # å¯ç”¨ GPU æ‹·è´
                    gpu_ar=self.gpu_ar,  # ğŸ”¥ ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„ gpu_ar
                    ratio=self.ratio,
                    is_sync=self.use_monitor,  # å¼‚æ­¥æ¨¡å¼
                    bsize=total_size_floats // 4,
                    memory_saving=True,
                    is_distributed=self.is_distributed,
                    rank=self.rank,
                    world_size=self.world_size
                )
                
                if self.verbose:
                    print(f"[Adapter] Chk_monitor åˆå§‹åŒ–æˆåŠŸ")
                    print(f"  - gpu_ar shape: {self.gpu_ar.shape}")
                    print(f"  - gpu_ar device: {self.gpu_ar.device}")
            else:
                # ä½¿ç”¨ Checkpoint ç›´æ¥æ¨¡å¼
                if self.verbose:
                    print(f"[Adapter] ä½¿ç”¨ Checkpoint ç›´æ¥æ¨¡å¼")
                
                self.pccheck_instance = Checkpoint(
                    total_size=total_size_floats,      # æ€»å¤§å°ï¼ˆfloat32 å…ƒç´ æ•°ï¼‰
                    num_threads=self.num_threads,       # çº¿ç¨‹æ•°
                    filename=self.checkpoint_file,      # æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
                    lib_path=self.c_lib_path,          # C åº“è·¯å¾„
                    max_async=self.max_async,           # æœ€å¤§å¹¶å‘æ£€æŸ¥ç‚¹æ•°
                    ratio=self.ratio,                   # CPUç¼“å†²åŒºå€æ•°
                    gpu_ar=self.gpu_ar,                 # ğŸ”¥ ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„ gpu_arï¼ˆå¯èƒ½ä¸º Noneï¼‰
                    bsize=self.batch_size_floats,      # æ‰¹æ¬¡å¤§å°ï¼ˆfloat32ï¼‰
                    memory_saving=True,                 # å¯ç”¨å†…å­˜èŠ‚çœæ¨¡å¼
                    is_distributed=self.is_distributed, # åˆ†å¸ƒå¼è®­ç»ƒæ ‡å¿—
                    rank=self.rank,                     # å½“å‰rank
                    world_size=self.world_size          # æ€»è¿›ç¨‹æ•°
                )
                
                # âš ï¸ å…³é”®ï¼šæ‰‹åŠ¨åˆå§‹åŒ– Writerï¼ˆå› ä¸º start_chk è®¾è®¡ç”¨äºåå°çº¿ç¨‹ï¼‰
                if Writer is not None:
                    total_mem_batches = int(self.ratio * total_size_floats / self.batch_size_floats)
                    self.pccheck_instance.writer = Writer(
                        self.checkpoint_file.encode(),
                        self.c_lib_path,
                        self.max_async,
                        int(self.batch_size_floats),
                        total_mem_batches,
                        self.is_distributed,
                        self.rank,
                        self.world_size
                    )
                    
                    if self.verbose:
                        print(f"[Adapter] Writer åˆå§‹åŒ–æˆåŠŸ (total_mem_batches={total_mem_batches})")
                        if self.gpu_ar is not None:
                            size_mb = self.gpu_ar.numel() * 4 / (1024**2)
                            print(f"  - ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„ gpu_ar: {size_mb:.2f} MB")
                
                # åˆ›å»ºé”å’ŒçŠ¶æ€å˜é‡ï¼ˆç”¨äºåŒæ­¥ï¼‰
                self.checkpoint_lock = Lock()
                self.cp_in_progress = Value('i', 0)
                
                if self.verbose:
                    print(f"[Adapter] Checkpoint åˆå§‹åŒ–æˆåŠŸ")
            
            if self.verbose:
                print(f"[Adapter] PCCheck åç«¯åˆå§‹åŒ–æˆåŠŸ")
                print(f"  - æ£€æŸ¥ç‚¹æ–‡ä»¶: {self.checkpoint_file}")
                print(f"  - C åº“è·¯å¾„: {self.c_lib_path}")
                
        except Exception as e:
            import traceback
            print(f"[Adapter] PCCheck åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"[Adapter] é”™è¯¯è¯¦æƒ…:")
            traceback.print_exc()
            print(f"[Adapter] åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼")
            self.use_pccheck = False
            self.use_monitor = False
            self._initialize_mock()
    
    def _initialize_mock(self):
        """åˆå§‹åŒ–æ¨¡æ‹Ÿåç«¯ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        if self.verbose:
            print(f"[Adapter] ä½¿ç”¨æ¨¡æ‹Ÿåç«¯ï¼ˆæ–‡ä»¶: {self.checkpoint_file}ï¼‰")
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹æ–‡ä»¶
        os.makedirs(os.path.dirname(self.checkpoint_file) or ".", exist_ok=True)
    
    def _get_checkpoint_filename(self, checkpoint_id: str, training_step: int) -> str:
        """
        ä¸ºæ¯ä¸ªæ£€æŸ¥ç‚¹ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
        
        Args:
            checkpoint_id: æ£€æŸ¥ç‚¹ ID
            training_step: è®­ç»ƒæ­¥æ•°
            
        Returns:
            å”¯ä¸€çš„æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        """
        # è·å–åŸºç¡€ç›®å½•å’Œæ–‡ä»¶æ‰©å±•å
        base_dir = os.path.dirname(self.checkpoint_file) or "."
        base_name = os.path.basename(self.checkpoint_file)
        name_parts = os.path.splitext(base_name)
        
        # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åï¼šcheckpoint_step_123.chk
        unique_filename = f"{name_parts[0]}_step_{training_step}{name_parts[1]}"
        unique_filepath = os.path.join(base_dir, unique_filename)
        
        return unique_filepath
    
    def allocate_staging_buffer(self, size_mb: float = 500.0):
        """
        åˆ†é… GPU bufferï¼ˆä»…åœ¨ gpu_ar æœªç”±å¤–éƒ¨ä¼ å…¥æ—¶ä½¿ç”¨ï¼‰
        
        Args:
            size_mb: Buffer å¤§å°ï¼ˆMBï¼‰
        """
        if self.gpu_ar is not None:
            return  # å·²ç»åˆ†é…
        
        size_bytes = int(size_mb * 1024 * 1024)
        size_elements = size_bytes // 4  # float32
        
        if torch.cuda.is_available():
            self.gpu_ar = torch.zeros(
                size_elements,
                dtype=torch.float32,
                device='cuda'
            )
            
            if self.verbose:
                print(f"[Adapter] åˆ†é… GPU buffer: {size_mb:.2f} MB")
        else:
            # CPU fallback
            self.gpu_ar = torch.zeros(
                size_elements,
                dtype=torch.float32
            )
            
            if self.verbose:
                print(f"[Adapter] åˆ†é… CPU buffer: {size_mb:.2f} MB")
    
    def save_layers_batch(self, tasks):
        """
        æ‰¹é‡ä¿å­˜å¤šä¸ªå±‚
        
        Args:
            tasks: List[SaveTask] æ¥è‡ªè°ƒåº¦å™¨
        """
        if not tasks:
            return
        
        start_time = time.time()
        
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šå¿«é€Ÿè·¯å¾„
        # å¦‚æœä½¿ç”¨ set_storageï¼Œå‚æ•°å·²åœ¨ gpu_ar ä¸­ï¼Œç›´æ¥ä¿å­˜æ•´ä¸ª bufferï¼ˆä¸€æ¬¡ I/Oï¼‰
        # é¿å…é€å±‚å¤åˆ¶å’Œå¤šæ¬¡åˆ·æ–°ï¼ˆåŸæœ¬ 148 æ¬¡ â†’ 1 æ¬¡ï¼‰
        if (hasattr(self, 'gpu_ar') and self.gpu_ar is not None and 
            self.use_pccheck and len(tasks) > 0):
            
            if self.verbose:
                total_size = sum(t.size_bytes for t in tasks)
                print(f"\n[Adapter] ğŸš€ ä½¿ç”¨å¿«é€Ÿè·¯å¾„ï¼ˆåŸå§‹ PCCheck æ¨¡å¼ï¼‰")
                print(f"  - ä»»åŠ¡æ•°: {len(tasks)}")
                print(f"  - ç†è®ºå¤§å°: {total_size / (1024**2):.2f} MB")
                print(f"  - å®é™…ä¿å­˜: {self.gpu_ar.numel() * 4 / (1024**2):.2f} MBï¼ˆæ•´ä¸ª gpu_arï¼‰")
            
            # ç›´æ¥ä¿å­˜æ•´ä¸ª gpu_arï¼ˆå‚æ•°å·²é€šè¿‡ set_storage åœ¨å…¶ä¸­ï¼‰
            self._save_entire_gpu_ar()
            
            elapsed = time.time() - start_time
            self.stats['total_layers_saved'] += len(tasks)
            self.stats['total_bytes_saved'] += sum(t.size_bytes for t in tasks)
            self.stats['total_save_time'] += elapsed
            
            if self.verbose:
                print(f"  âœ… å¿«é€Ÿè·¯å¾„å®Œæˆï¼Œè€—æ—¶: {elapsed*1000:.2f} ms")
            
            return
        
        # å¦åˆ™ä½¿ç”¨åŸæœ‰çš„åˆ†å±‚ä¿å­˜é€»è¾‘
        if self.verbose:
            total_size = sum(t.size_bytes for t in tasks)
            print(f"\n[Adapter] å¼€å§‹ä¿å­˜æ‰¹æ¬¡ï¼ˆæ ‡å‡†æ¨¡å¼ï¼‰")
            print(f"  - ä»»åŠ¡æ•°: {len(tasks)}")
            print(f"  - æ€»å¤§å°: {total_size / (1024**2):.2f} MB")
        
        # ä½¿ç”¨ gpu_ar èšåˆåæ‰¹é‡ä¿å­˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.gpu_ar is not None:
            self._save_via_staging_buffer(tasks)
        else:
            # æ–¹æ¡ˆ2ï¼šé€å±‚ä¿å­˜
            self._save_layer_by_layer(tasks)
        
        elapsed = time.time() - start_time
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats['total_layers_saved'] += len(tasks)
        self.stats['total_bytes_saved'] += sum(t.size_bytes for t in tasks)
        self.stats['total_save_time'] += elapsed
        
        if self.verbose:
            throughput = sum(t.size_bytes for t in tasks) / (1024**2) / elapsed
            print(f"  - ä¿å­˜è€—æ—¶: {elapsed*1000:.2f} ms")
            print(f"  - ååé‡: {throughput:.2f} MB/s")
    
    def _save_entire_gpu_ar(self, checkpoint_file: Optional[str] = None):
        """
        å¿«é€Ÿè·¯å¾„ï¼šä¸€æ¬¡æ€§ä¿å­˜æ•´ä¸ª gpu_ar
        
        é€‚ç”¨åœºæ™¯ï¼š
        - å‚æ•°é€šè¿‡ set_storage é‡å®šå‘åˆ° gpu_ar
        - æ‰€æœ‰å‚æ•°å·²åœ¨è¿ç»­çš„ GPU å†…å­˜ä¸­
        - é¿å…é€å±‚å¤åˆ¶å’Œå¤šæ¬¡ I/O
        
        æ€§èƒ½ï¼š
        - I/O æ¬¡æ•°ï¼š1 æ¬¡ï¼ˆvs åˆ†å±‚çš„ 148 æ¬¡ï¼‰
        - å¤åˆ¶æ¬¡æ•°ï¼š0 æ¬¡ï¼ˆvs åˆ†å±‚çš„å¤šæ¬¡ GPU-CPU-GPU å¤åˆ¶ï¼‰
        - é¢„æœŸæ—¶é—´ï¼š~5-10msï¼ˆvs åˆ†å±‚çš„ ~10,000msï¼‰
        
        Args:
            checkpoint_file: å¯é€‰çš„æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ã€‚å¦‚æœä¸º Noneï¼Œä½¿ç”¨ self.checkpoint_file
        """
        if not self.use_pccheck:
            if self.verbose:
                print(f"[Adapter] âš ï¸ PCCheck æœªå¯ç”¨ï¼Œè·³è¿‡ä¿å­˜")
            return
        
        # ä½¿ç”¨ä¼ å…¥çš„æ–‡ä»¶åï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤çš„
        target_file = checkpoint_file if checkpoint_file is not None else self.checkpoint_file
        
        total_size = self.gpu_ar.numel()
        
        if self.verbose:
            size_mb = total_size * 4 / (1024**2)
            print(f"[Adapter] ğŸ’¾ ä¿å­˜æ•´ä¸ª gpu_ar: {size_mb:.2f} MB ({total_size:,} params)")
            print(f"[Adapter]    åˆ°æ–‡ä»¶: {target_file}")
        
        try:
            if self.use_monitor and self.pccheck_monitor is not None:
                # ä½¿ç”¨ Chk_monitorï¼ˆåå°è¿›ç¨‹æ¨¡å¼ï¼Œæ›´é«˜æ•ˆï¼‰
                if self.verbose:
                    print(f"[Adapter] ä½¿ç”¨ Monitor æ¨¡å¼ï¼ˆå¼‚æ­¥ï¼‰")
                
                # âš ï¸ Monitor æ¨¡å¼ï¼šéœ€è¦æ›´æ–°æ–‡ä»¶è·¯å¾„
                # æ³¨æ„ï¼šMonitor åœ¨åˆå§‹åŒ–æ—¶å·²ç»è®¾ç½®äº†æ–‡ä»¶è·¯å¾„ï¼Œè¿™é‡Œéœ€è¦ç‰¹æ®Šå¤„ç†
                # å¦‚æœ Monitor ä¸æ”¯æŒåŠ¨æ€æ›´æ”¹æ–‡ä»¶è·¯å¾„ï¼Œåˆ™éœ€è¦é‡æ–°åˆ›å»º Monitor å®ä¾‹
                # æˆ–è€…åœ¨å¤–éƒ¨ä¿è¯æ¯æ¬¡è°ƒç”¨éƒ½ä½¿ç”¨ä¸€è‡´çš„æ–‡ä»¶å
                
                # ğŸš€ å…³é”®ä¼˜åŒ–ï¼šè§¦å‘å¼‚æ­¥ä¿å­˜ï¼Œç«‹å³è¿”å›
                # Monitor åå°è¿›ç¨‹ä¼šå¤„ç†å®é™…çš„ä¿å­˜å·¥ä½œ
                # è®­ç»ƒä¸éœ€è¦ç­‰å¾…ä¿å­˜å®Œæˆï¼
                self.pccheck_monitor.save()
                
                # âœ… ç«‹å³è¿”å›ï¼Œä¸ç­‰å¾…ä¿å­˜å®Œæˆ
                # åŸæ¥çš„ä»£ç ä¼šç­‰å¾…ï¼Œå¯¼è‡´ 273ms é˜»å¡ï¼š
                # while self.pccheck_monitor.checkpoint_in_progress():
                #     time.sleep(0.001)
                
                if self.verbose:
                    print(f"[Adapter] âœ… å¼‚æ­¥ä¿å­˜å·²è§¦å‘ï¼ˆåå°è¿›è¡Œï¼‰")
                
            elif self.pccheck_instance is not None:
                # ä½¿ç”¨ Checkpoint ç›´æ¥æ¨¡å¼
                if self.verbose:
                    print(f"[Adapter] ä½¿ç”¨ç›´æ¥æ¨¡å¼")
                
                # ğŸ”¥ ä¿®å¤ï¼šå¦‚æœæä¾›äº†æ–°çš„æ–‡ä»¶åï¼Œéœ€è¦æ›´æ–° Writer
                if target_file != self.checkpoint_file and Writer is not None:
                    # é‡æ–°åˆ›å»º Writer å®ä¾‹ä»¥ä½¿ç”¨æ–°çš„æ–‡ä»¶å
                    total_mem_batches = int(self.ratio * total_size / self.batch_size_floats)
                    self.pccheck_instance.writer = Writer(
                        target_file.encode(),
                        self.c_lib_path,
                        self.max_async,
                        int(self.batch_size_floats),
                        total_mem_batches,
                        self.is_distributed,
                        self.rank,
                        self.world_size
                    )
                    if self.verbose:
                        print(f"[Adapter] Writer å·²æ›´æ–°åˆ°æ–°æ–‡ä»¶: {target_file}")
                
                # ç›´æ¥è°ƒç”¨åŸå§‹ PCCheck çš„ write_pipelined
                self.pccheck_instance.write_pipelined(
                    cpu_ar=None,  # å†…éƒ¨åˆ†é… CPU ç¼“å†²åŒºï¼ˆmemory_saving=Trueï¼‰
                    num_threads=self.num_threads,
                    sz=total_size,
                    batch_size=self.batch_size_floats,
                    ratio=self.ratio,
                    memory_saving=True,
                    is_distributed=self.is_distributed,
                    rank=self.rank,
                    world_size=self.world_size
                )
                
            if self.verbose:
                print(f"[Adapter] âœ… ä¿å­˜å®Œæˆ")
                
        except Exception as e:
            print(f"[Adapter] âŒ ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def save_entire_checkpoint(self, checkpoint_id: str, training_step: int):
        """
        æ‰¹é‡ä¿å­˜æ¥å£ï¼šç›´æ¥ä¿å­˜æ•´ä¸ªæ£€æŸ¥ç‚¹ï¼ˆä¸ç»è¿‡è°ƒåº¦å™¨ï¼‰
        
        Args:
            checkpoint_id: æ£€æŸ¥ç‚¹ ID
            training_step: è®­ç»ƒæ­¥æ•°
        
        é€‚ç”¨åœºæ™¯ï¼š
        - å‚æ•°é€šè¿‡ set_storage åœ¨ gpu_ar ä¸­
        - è·³è¿‡ç»†ç²’åº¦è°ƒåº¦ï¼Œç›´æ¥ä¿å­˜æ•´ä¸ª buffer
        
        æ€§èƒ½ä¼˜åŠ¿ï¼š
        - 0 æ¬¡å›è°ƒå¼€é”€ï¼ˆvs 148 æ¬¡ï¼‰
        - 1 æ¬¡ I/Oï¼ˆvs 5-7 æ¬¡ï¼‰
        - å¤§å—å†™å…¥ï¼ˆ500MBï¼‰ï¼Œé¥±å’Œå¸¦å®½
        """
        if self.gpu_ar is None or not self.use_pccheck:
            if self.verbose:
                print(f"[Adapter] è·³è¿‡ä¿å­˜ï¼ˆgpu_ar={self.gpu_ar is not None}, use_pccheck={self.use_pccheck})")
            return
        
        start_time = time.time()
        
        # ğŸ”¥ ä¿®å¤ï¼šä¸ºæ¯ä¸ªæ£€æŸ¥ç‚¹ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
        checkpoint_file_for_this_step = self._get_checkpoint_filename(checkpoint_id, training_step)
        
        if self.verbose:
            size_mb = self.gpu_ar.numel() * 4 / (1024**2)
            print(f"\n[Adapter] ğŸ’¾ æ‰¹é‡ä¿å­˜æ£€æŸ¥ç‚¹")
            print(f"  - Checkpoint ID: {checkpoint_id}")
            print(f"  - Training Step: {training_step}")
            print(f"  - æ–‡ä»¶: {checkpoint_file_for_this_step}")
            print(f"  - å¤§å°: {size_mb:.2f} MB ({self.gpu_ar.numel():,} params)")
        
        try:
            # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨ç‰¹å®šçš„æ–‡ä»¶åä¿å­˜ï¼ˆä¸€æ¬¡ I/Oï¼‰
            self._save_entire_gpu_ar(checkpoint_file_for_this_step)
            
            elapsed = time.time() - start_time
            
            if self.verbose:
                throughput_mbs = (self.gpu_ar.numel() * 4 / (1024**2)) / elapsed
                print(f"  âœ… ä¿å­˜å®Œæˆ")
                print(f"    - è€—æ—¶: {elapsed*1000:.2f} ms")
                print(f"    - ååé‡: {throughput_mbs:.2f} MB/s")
            
            # æ›´æ–°ç»Ÿè®¡
            self.stats['total_bytes_saved'] += self.gpu_ar.numel() * 4
            self.stats['total_save_time'] += elapsed
            
        except Exception as e:
            print(f"[Adapter] âŒ æ‰¹é‡ä¿å­˜å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    def save_entire_checkpoint_in_chunks(self, checkpoint_id: str, training_step: int, chunk_count: int = 2):
        """
        å°†æ•´ä¸ª gpu_ar åˆ’åˆ†ä¸ºè‹¥å¹² chunk å¹¶è¡Œä¿å­˜ã€‚

        è¯´æ˜ï¼š
        - åœ¨æ— æ³•ä½¿ç”¨ Monitor çš„æƒ…å†µä¸‹ï¼ˆæˆ–å³ä½¿å¯ç”¨ï¼‰ï¼Œå°†å¤§å—æ•°æ®åˆ’åˆ†ä¸ºè‹¥å¹²å¹¶å‘å†™å…¥
          å¯ä»¥æ›´å¥½åœ°é¥±å’Œ I/O å¸¦å®½ï¼Œå¹¶å‡å°‘å•æ¬¡é˜»å¡æ—¶é—´ã€‚
        - è¯¥å®ç°ä¼šä¸ºæ¯ä¸ª chunk å¯åŠ¨ä¸€ä¸ªçº¿ç¨‹ï¼Œçº¿ç¨‹å†…ä¼šæ‹·è´å¯¹åº”çš„ slice åˆ°
          PCCheck çš„ gpu_ar æˆ– staging buffer å¹¶è°ƒç”¨åº•å±‚å†™å…¥æ¥å£ã€‚

        æ³¨æ„ï¼šåº•å±‚ PCCheck å®ç°éœ€è¦æ”¯æŒåŒæ—¶è¿›è¡Œå¤šä¸ª write_pipelined è°ƒç”¨
       ï¼ˆé€šè¿‡ writer.max_async ç­‰å‚æ•°æ§åˆ¶ï¼‰ã€‚å¦‚æœåº•å±‚ä¸æ”¯æŒå¹¶å‘å†™å…¥ï¼Œ
        å¹¶å‘å†™å…¥å¯èƒ½ä¼šè¢«å†…éƒ¨æ’é˜Ÿæˆ–å˜å¾—ä¸ç¨³å®šï¼Œè¯·æ®å®é™…æƒ…å†µè°ƒæ•´ chunk_countã€‚
        """
        if self.gpu_ar is None or not self.use_pccheck:
            if self.verbose:
                print(f"[Adapter] è·³è¿‡åˆ†å—ä¿å­˜ï¼ˆgpu_ar={self.gpu_ar is not None}, use_pccheck={self.use_pccheck})")
            return

        # ğŸ”¥ ä¿®å¤ï¼šä¸ºæ¯ä¸ªæ£€æŸ¥ç‚¹ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å
        checkpoint_file_for_this_step = self._get_checkpoint_filename(checkpoint_id, training_step)

        total_floats = int(self.gpu_ar.numel())
        if total_floats == 0:
            return

        # è®¡ç®—æ¯ä¸ª chunk çš„å¤§å°ï¼ˆä»¥ float ä¸ºå•ä½ï¼‰
        import math
        chunk_size = int(math.ceil(total_floats / float(max(1, chunk_count))))

        if self.verbose:
            size_mb = total_floats * 4 / (1024**2)
            print(f"\n[Adapter] ğŸ’¾ åˆ†å—ä¿å­˜æ£€æŸ¥ç‚¹: {chunk_count} chunks, æ€»å¤§å°: {size_mb:.2f} MB")
            print(f"  - æ–‡ä»¶: {checkpoint_file_for_this_step}")

        threads = []

        def _save_chunk(start_idx: int, end_idx: int, idx: int):
            try:
                num_elems = end_idx - start_idx
                if num_elems <= 0:
                    return

                if self.verbose:
                    print(f"  [Chunk {idx}] ä¿å­˜èŒƒå›´: {start_idx}:{end_idx} ({num_elems} floats)")

                # ç›´æ¥ä» gpu_ar çš„ slice è¿›è¡Œå†™å…¥
                chunk_tensor = self.gpu_ar[start_idx:end_idx]

                # ä¸´æ—¶æ‹·è´åˆ° pccheck_instance.gpu_ar æˆ– Monitor buffer å¹¶è°ƒç”¨å†™å…¥
                if self.use_monitor and self.pccheck_monitor is not None:
                    # å°†æ•°æ®å¤åˆ¶åˆ° gpu_arï¼ˆMonitor ä¼šä½¿ç”¨å®ƒï¼‰
                    if self.gpu_ar is not None and num_elems <= self.gpu_ar.numel():
                        self.gpu_ar[:num_elems].copy_(chunk_tensor)
                    # è§¦å‘ Monitor çš„ä¿å­˜ï¼ˆå¼‚æ­¥ï¼‰
                    # âš ï¸ æ³¨æ„ï¼šMonitor ä¸æ”¯æŒåˆ†å—ä¿å­˜åˆ°ä¸åŒæ–‡ä»¶ï¼Œæ‰€ä»¥è¿™é‡Œä¸é€‚ç”¨
                    self.pccheck_monitor.save()
                    if self.verbose:
                        print(f"  [Chunk {idx}] Monitor.save() å·²è§¦å‘")
                else:
                    # ç›´æ¥æ¨¡å¼ï¼šå¤åˆ¶åˆ° pccheck_instance.gpu_ar å¹¶è°ƒç”¨ write_pipelined
                    if self.pccheck_instance.gpu_ar is None or self.pccheck_instance.gpu_ar.numel() < num_elems:
                        # åˆ†é…æˆ–æ‰©å±•ç›®æ ‡ç¼“å†²åŒº
                        self.pccheck_instance.gpu_ar = torch.zeros(
                            num_elems,
                            dtype=torch.float32,
                            device='cuda' if torch.cuda.is_available() else 'cpu'
                        )

                    # å¤åˆ¶ chunk åˆ°ç›®æ ‡ gpu_ar
                    self.pccheck_instance.gpu_ar[:num_elems].copy_(chunk_tensor)
                    
                    # ğŸ”¥ ä¿®å¤ï¼šä¸ºè¿™ä¸ª chunk åˆ›å»ºæˆ–æ›´æ–° Writer
                    # æ³¨æ„ï¼šåˆ†å—ä¿å­˜å®é™…ä¸Šè¿˜æ˜¯ä¿å­˜åˆ°åŒä¸€ä¸ªæ–‡ä»¶ï¼Œåªæ˜¯åˆ†æ‰¹å†™å…¥
                    if Writer is not None and self.pccheck_instance.writer is None:
                        total_mem_batches = int(self.ratio * num_elems / self.batch_size_floats)
                        self.pccheck_instance.writer = Writer(
                            checkpoint_file_for_this_step.encode(),
                            self.c_lib_path,
                            self.max_async,
                            int(self.batch_size_floats),
                            total_mem_batches,
                            self.is_distributed,
                            self.rank,
                            self.world_size
                        )

                    # è°ƒç”¨å†™å…¥ï¼ˆwrite_pipelined å†…éƒ¨åº”å¤„ç†å¹¶å‘ï¼‰
                    self.pccheck_instance.write_pipelined(
                        cpu_ar=None,
                        num_threads=self.num_threads,
                        sz=num_elems,
                        bsize=self.batch_size_floats,
                        lock=self.checkpoint_lock,
                        cp_in_progress=self.cp_in_progress
                    )
                    if self.verbose:
                        print(f"  [Chunk {idx}] write_pipelined å®Œæˆ (åŒæ­¥è¿”å›/æˆ–å†…éƒ¨æ’é˜Ÿ)")

                # ç»Ÿè®¡
                self.stats['total_bytes_saved'] += num_elems * 4

            except Exception as e:
                print(f"[Adapter] Chunk {idx} ä¿å­˜å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

        # å¯åŠ¨çº¿ç¨‹ä¿å­˜æ¯ä¸ª chunk
        for i in range(chunk_count):
            s = i * chunk_size
            e = min((i + 1) * chunk_size, total_floats)
            t = threading.Thread(target=_save_chunk, args=(s, e, i), daemon=True)
            t.start()
            threads.append(t)

        # å¯é€‰ï¼šä¸ç­‰å¾…æ‰€æœ‰ chunk å®Œæˆï¼Œä»¥ä¾¿è®­ç»ƒå¯ä»¥ç»§ç»­ï¼ˆæ›´æ¿€è¿›çš„å¹¶å‘ç­–ç•¥ï¼‰
        # è¿™é‡Œæˆ‘ä»¬é€‰æ‹©ä¸é˜»å¡ï¼ˆä¸ Monitor æ¨¡å¼ä¸€è‡´ï¼‰ï¼Œä½†å¦‚æœéœ€è¦ç¡®ä¿å†™å…¥å®Œæˆå†ç»§ç»­ï¼Œå¯ join()
        if self.verbose:
            print(f"[Adapter] å·²å¹¶è¡Œè§¦å‘ {len(threads)} ä¸ª chunk ä¿å­˜çº¿ç¨‹ï¼ˆä¸ç­‰å¾…å®Œæˆï¼‰")

    
    def _save_via_staging_buffer(self, tasks):
        """
        é€šè¿‡ gpu_ar ä¿å­˜ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        
        ğŸ”¥ ä¼˜åŒ–ï¼šç›´æ¥ä»åŸå§‹å‚æ•°æ‹·è´åˆ° gpu_arï¼Œé¿å…ä¸­é—´ç¼“å†²
        ç”±äº SaveTask.parameters ç°åœ¨åªå­˜å‚¨å¼•ç”¨ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä»åŸåœ°å€æ‹·è´
        """
        buffer_offset = 0
        
        # ğŸ”¥ å…³é”®ï¼šæ™ºèƒ½å¤åˆ¶ç­–ç•¥
        # ç­–ç•¥ 1ï¼šå¦‚æœä½¿ç”¨ set_storageï¼Œå‚æ•°å·²ç»åœ¨ gpu_ar ä¸­ï¼Œå¯ä»¥ç›´æ¥ä¿å­˜æ•´ä¸ª gpu_arï¼ˆé›¶æ‹·è´ï¼‰
        # ç­–ç•¥ 2ï¼šå¦åˆ™ï¼Œéœ€è¦é€ä¸ªå¤åˆ¶å‚æ•°åˆ° gpu_ar
        
        for task in tasks:
            for param in task.parameters:
                # å±•å¹³å‚æ•°ï¼ˆviewï¼Œä¸äº§ç”Ÿæ‹·è´ï¼‰
                param_flat = param.flatten()
                param_size = param_flat.numel()
                
                # æ£€æŸ¥ç©ºé—´
                if buffer_offset + param_size > self.gpu_ar.numel():
                    # Buffer å·²æ»¡ï¼Œå…ˆåˆ·æ–°å½“å‰å†…å®¹
                    self._flush_buffer(buffer_offset)
                    buffer_offset = 0
                
                end_offset = buffer_offset + param_size
                target_buffer = self.gpu_ar[buffer_offset:end_offset]
                
                # æ™ºèƒ½æ‹·è´ä¼˜åŒ–
                param_ptr = param_flat.data_ptr()
                target_ptr = target_buffer.data_ptr()
                
                if param_ptr == target_ptr:
                    # ğŸ¯ æœ€ä¼˜æƒ…å†µï¼šå‚æ•°æ­£å¥½åœ¨ç›®æ ‡ä½ç½®ï¼ˆé›¶æ‹·è´ï¼‰
                    # è¿™å‘ç”Ÿåœ¨ä½¿ç”¨ set_storage ä¸”å‚æ•°æŒ‰é¡ºåºä¿å­˜æ—¶
                    if self.verbose:
                        print(f"[Adapter] âš¡ é›¶æ‹·è´: {task.layer_name}")
                else:
                    # éœ€è¦å¤åˆ¶æ•°æ®
                    buffer_base_ptr = self.gpu_ar.data_ptr()
                    buffer_size_bytes = self.gpu_ar.numel() * self.gpu_ar.element_size()
                    
                    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å†…å­˜é‡å ï¼ˆé¿å…è‡ªæˆ‘è¦†ç›–ï¼‰
                    param_in_buffer = (
                        param_ptr >= buffer_base_ptr and 
                        param_ptr < buffer_base_ptr + buffer_size_bytes
                    )
                    
                    if param_in_buffer:
                        # å‚æ•°åœ¨ buffer ä¸­ä½†ä½ç½®ä¸å¯¹ï¼Œéœ€è¦ clone é¿å…è‡ªæˆ‘è¦†ç›–
                        target_buffer.copy_(param_flat.clone())
                        if self.verbose:
                            print(f"[Adapter] ğŸ”„ Clone å¤åˆ¶: {task.layer_name} (å†…å­˜é‡å )")
                    else:
                        # æ­£å¸¸å¤åˆ¶ï¼ˆæ— å†…å­˜é‡å ï¼‰
                        target_buffer.copy_(param_flat)
                        if self.verbose:
                            print(f"[Adapter] ğŸ“‹ æ­£å¸¸å¤åˆ¶: {task.layer_name}")
                
                # è®°å½•å…ƒæ•°æ®
                metadata = LayerMetadata(
                    layer_name=task.layer_name,
                    training_step=task.training_step,
                    checkpoint_id=task.checkpoint_id,
                    offset_in_file=self.current_file_offset,
                    size_bytes=param.numel() * param.element_size(),
                    param_count=param.numel(),
                    shapes=[tuple(param.shape)],
                    dtypes=[str(param.dtype)],
                    timestamp=task.timestamp
                )
                self.layer_metadata.append(metadata)
                
                buffer_offset = end_offset
                self.current_file_offset += param.numel() * param.element_size()
        
        # åˆ·æ–°å‰©ä½™æ•°æ®
        if buffer_offset > 0:
            self._flush_buffer(buffer_offset)
    
    def _flush_buffer(self, buffer_size: int):
        """å°† gpu_ar çš„æ•°æ®å†™å…¥å­˜å‚¨"""
        if buffer_size == 0:
            return
        
        # è·å–æœ‰æ•ˆæ•°æ®éƒ¨åˆ†
        valid_data = self.gpu_ar[:buffer_size]
        
        if self.use_pccheck:
            # ä½¿ç”¨ PCCheck ä¿å­˜
            self._save_to_pccheck(valid_data)
        else:
            # æ¨¡æ‹Ÿä¿å­˜ï¼ˆå†™å…¥æ–‡ä»¶ï¼‰
            self._save_to_file(valid_data)
    
    def _save_to_pccheck(self, data: torch.Tensor):
        """ä½¿ç”¨ PCCheck ä¿å­˜æ•°æ®"""
        if self.pccheck_instance is None and self.pccheck_monitor is None:
            print("[Adapter] é”™è¯¯: PCCheck å®ä¾‹æœªåˆå§‹åŒ–")
            return
        
        try:
            # å°†æ•°æ®è½¬æ¢ä¸º CPU numpy æ•°ç»„
            # data_cpu = data.cpu().numpy().astype(np.float32)
            data_cpu = data.detach().cpu().float().numpy()
            
            # è®¡ç®—æ‰¹æ¬¡ä¿¡æ¯
            total_size = data_cpu.size
            num_batches = (total_size + self.batch_size_floats - 1) // self.batch_size_floats
            
            if self.verbose:
                size_mb = total_size * 4 / (1024**2)
                print(f"    [PCCheck] å†™å…¥ {size_mb:.2f} MB ({total_size:,} floats)")
                print(f"    [PCCheck] åˆ† {num_batches} ä¸ªæ‰¹æ¬¡å†™å…¥")
            
            if self.use_monitor and self.pccheck_monitor is not None:
                # ä½¿ç”¨ Chk_monitorï¼ˆåå°è¿›ç¨‹æ¨¡å¼ï¼‰
                if self.verbose:
                    print(f"    [PCCheck] ä½¿ç”¨ Monitor æ¨¡å¼ä¿å­˜")
                
                # å°†æ•°æ®å¤åˆ¶åˆ° GPU buffer
                if self.gpu_ar is not None:
                    self.gpu_ar[:data.numel()].copy_(data)
                
                # ç­‰å¾…ä¹‹å‰çš„æ£€æŸ¥ç‚¹å®Œæˆ
                while self.pccheck_monitor.checkpoint_in_progress():
                    time.sleep(0.001)
                
                # è§¦å‘ä¿å­˜
                self.pccheck_monitor.save()
                
                # ç­‰å¾… GPU æ‹·è´å®Œæˆï¼ˆå¯é€‰ï¼Œå–å†³äºæ˜¯å¦éœ€è¦ç«‹å³é‡ç”¨ bufferï¼‰
                while self.pccheck_monitor.gpu_copy_in_progress():
                    time.sleep(0.001)
                
            else:
                # ä½¿ç”¨ Checkpoint ç›´æ¥æ¨¡å¼
                if self.verbose:
                    print(f"    [PCCheck] ä½¿ç”¨ç›´æ¥æ¨¡å¼ä¿å­˜")
                
                # âš ï¸ å…³é”®ï¼šå…ˆå°†æ•°æ®å¤åˆ¶åˆ° Checkpoint çš„ gpu_ar
                # å¦‚æœ gpu_ar è¿˜æœªåˆ†é…æˆ–å¤§å°ä¸å¤Ÿï¼Œåˆ™é‡æ–°åˆ†é…
                if self.pccheck_instance.gpu_ar is None or self.pccheck_instance.gpu_ar.numel() < total_size:
                    if self.verbose:
                        print(f"    [PCCheck] åˆ†é… GPU buffer: {total_size:,} floats")
                    self.pccheck_instance.gpu_ar = torch.zeros(
                        total_size, 
                        dtype=torch.float32, 
                        device='cuda' if torch.cuda.is_available() else 'cpu'
                    )
                
                # å°†æˆ‘ä»¬çš„æ•°æ®å¤åˆ¶åˆ° Checkpoint çš„ gpu_ar
                self.pccheck_instance.gpu_ar[:total_size].copy_(data)
                
                # ä½¿ç”¨ PCCheck çš„ write_pipelined æ–¹æ³•
                # æ³¨æ„ï¼šæ ¹æ® chk_checkpoint_pipeline.pyï¼Œè¿™ä¸ªæ–¹æ³•ä¼šå¯åŠ¨å¤šçº¿ç¨‹è¿›è¡Œæµæ°´çº¿å†™å…¥
                self.pccheck_instance.write_pipelined(
                    cpu_ar=None,  # ä½¿ç”¨ PCCheck å†…éƒ¨åˆ†é…çš„ CPU ç¼“å†²åŒºï¼ˆmemory_saving=Trueï¼‰
                    num_threads=self.num_threads,
                    sz=total_size,
                    bsize=self.batch_size_floats,
                    lock=self.checkpoint_lock,
                    cp_in_progress=self.cp_in_progress
                )
            
            if self.verbose:
                print(f"    [PCCheck] å†™å…¥å®Œæˆ")
                
        except Exception as e:
            import traceback
            print(f"[Adapter] PCCheck å†™å…¥å¤±è´¥: {e}")
            traceback.print_exc()
            print(f"[Adapter] å›é€€åˆ°æ–‡ä»¶å†™å…¥æ¨¡å¼")
            self._save_to_file(data)
    
    def _save_to_file(self, data: torch.Tensor):
        """æ¨¡æ‹Ÿä¿å­˜ï¼šå†™å…¥äºŒè¿›åˆ¶æ–‡ä»¶"""
        # è½¬æ¢ä¸º numpy å¹¶å†™å…¥
        data_cpu = data.detach().cpu().float().numpy()
        
        with open(self.checkpoint_file, 'ab') as f:
            data_cpu.tofile(f)
        
        if self.verbose:
            size_mb = data.numel() * 4 / (1024**2)
            print(f"    [Mock] å†™å…¥ {size_mb:.2f} MB åˆ°æ–‡ä»¶")
    
    def _save_layer_by_layer(self, tasks):
        """é€å±‚ä¿å­˜ï¼ˆä¸ä½¿ç”¨ staging bufferï¼‰"""
        for task in tasks:
            for param in task.parameters:
                # è½¬æ¢ä¸ºè¿ç»­å­˜å‚¨
                param_contiguous = param.contiguous()
                
                # è®°å½•å…ƒæ•°æ®
                metadata = LayerMetadata(
                    layer_name=task.layer_name,
                    training_step=task.training_step,
                    checkpoint_id=task.checkpoint_id,
                    offset_in_file=self.current_file_offset,
                    size_bytes=param.numel() * param.element_size(),
                    param_count=param.numel(),
                    shapes=[tuple(param.shape)],
                    dtypes=[str(param.dtype)],
                    timestamp=task.timestamp
                )
                self.layer_metadata.append(metadata)
                
                # ä¿å­˜æ•°æ®
                if self.use_pccheck:
                    self._save_to_pccheck(param_contiguous.flatten())
                else:
                    self._save_to_file(param_contiguous.flatten())
                
                self.current_file_offset += param.numel() * param.element_size()
    
    def save_metadata(self):
        """ä¿å­˜å…ƒæ•°æ®åˆ° JSON æ–‡ä»¶"""
        metadata_dict = {
            'checkpoint_file': self.checkpoint_file,
            'total_layers': len(self.layer_metadata),
            'total_size_bytes': sum(m.size_bytes for m in self.layer_metadata),
            'layers': [
                {
                    'layer_name': m.layer_name,
                    'training_step': m.training_step,
                    'checkpoint_id': m.checkpoint_id,
                    'offset_in_file': m.offset_in_file,
                    'size_bytes': m.size_bytes,
                    'param_count': m.param_count,
                    'shapes': m.shapes,
                    'dtypes': m.dtypes,
                    'timestamp': m.timestamp
                }
                for m in self.layer_metadata
            ]
        }
        
        with open(self.metadata_file, 'w') as f:
            json.dump(metadata_dict, f, indent=2)
        
        if self.verbose:
            print(f"\n[Adapter] å…ƒæ•°æ®å·²ä¿å­˜: {self.metadata_file}")
            print(f"  - æ€»å±‚æ•°: {metadata_dict['total_layers']}")
            print(f"  - æ€»å¤§å°: {metadata_dict['total_size_bytes'] / (1024**3):.2f} GB")
    
    def shutdown(self):
        """å…³é—­é€‚é…å™¨"""
        if self.verbose:
            print(f"\n[Adapter] æ­£åœ¨å…³é—­...")
        
        # ç­‰å¾…æ‰€æœ‰æ£€æŸ¥ç‚¹å®Œæˆ
        if self.use_monitor and self.pccheck_monitor is not None:
            # ç­‰å¾… Monitor ä¸­çš„æ£€æŸ¥ç‚¹å®Œæˆ
            while self.pccheck_monitor.checkpoint_in_progress():
                time.sleep(0.01)
            
            # å…³é—­ Monitor
            if self.verbose:
                print(f"[Adapter] å…³é—­ PCCheck Monitor...")
            self.pccheck_monitor.kill_checkpoint()
        
        # ä¿å­˜å…ƒæ•°æ®
        self.save_metadata()
        
        # é‡Šæ”¾èµ„æº
        if self.gpu_ar is not None:
            del self.gpu_ar
            self.gpu_ar = None
        
        if self.cpu_buffer is not None:
            del self.cpu_buffer
            self.cpu_buffer = None
        
        # æ‰“å°ç»Ÿè®¡
        self.print_stats()
        
        if self.verbose:
            print(f"[Adapter] å·²å…³é—­")
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n{'='*80}")
        print(f"PCCheck é€‚é…å™¨ç»Ÿè®¡")
        print(f"{'='*80}")
        print(f"æ€»ä¿å­˜å±‚æ•°: {self.stats['total_layers_saved']}")
        print(f"æ€»ä¿å­˜æ•°æ®é‡: {self.stats['total_bytes_saved'] / (1024**3):.2f} GB")
        print(f"æ€»ä¿å­˜æ—¶é—´: {self.stats['total_save_time']:.2f} ç§’")
        
        if self.stats['total_save_time'] > 0:
            throughput = self.stats['total_bytes_saved'] / (1024**2) / self.stats['total_save_time']
            print(f"å¹³å‡ååé‡: {throughput:.2f} MB/s")
        print(f"{'='*80}\n")
